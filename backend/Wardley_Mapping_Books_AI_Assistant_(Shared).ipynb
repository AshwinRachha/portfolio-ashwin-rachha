{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up\n"
      ],
      "metadata": {
        "id": "0UqlAAxTXnGF"
      },
      "id": "0UqlAAxTXnGF"
    },
    {
      "cell_type": "code",
      "source": [
        "# Core Requirements\n",
        "!pip install -q langchain\n",
        "!pip install -q openai\n",
        "!pip install -q tiktoken\n",
        "!pip install -q pdfminer.six\n",
        "!pip install -q unstructured\n",
        "!pip install -q bs4"
      ],
      "metadata": {
        "id": "0wXix_o85iba",
        "outputId": "351f1648-7c8e-4fe8-a6bd-c2c718852467",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0wXix_o85iba",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers"
      ],
      "metadata": {
        "id": "HDhJ0oKXFxRB",
        "outputId": "23e10534-5c33-4a46-d889-8aa4af6e1c36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "HDhJ0oKXFxRB",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m81.9/86.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FAISS vector database\n",
        "!pip install -q faiss-cpu\n",
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "71fF2lhe53Qa",
        "outputId": "81fc97b8-5dd2-403d-bbfa-3fcb7326c976",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "71fF2lhe53Qa",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up OPEN_API_KEY and necessary variables"
      ],
      "metadata": {
        "id": "TUbD-eXCXNy-"
      },
      "id": "TUbD-eXCXNy-"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\" # Put you OpenAI API Key here"
      ],
      "metadata": {
        "id": "tY7CJZoh5cma"
      },
      "id": "tY7CJZoh5cma",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import re\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "from bs4 import BeautifulSoup as Soup\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import (\n",
        "    TextLoader,\n",
        "    WebBaseLoader,\n",
        "    UnstructuredFileLoader,\n",
        "    UnstructuredURLLoader,\n",
        "    PyPDFLoader,\n",
        "    OnlinePDFLoader\n",
        "    )\n",
        "\n",
        "from langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.chains import VectorDBQAWithSourcesChain"
      ],
      "metadata": {
        "id": "sTnK2sjt6JGT"
      },
      "id": "sTnK2sjt6JGT",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clone the GitHub repo and fetch the source docs to create the vector database"
      ],
      "metadata": {
        "id": "Kwk0Vu46YWjQ"
      },
      "id": "Kwk0Vu46YWjQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the datastore"
      ],
      "metadata": {
        "id": "1mHZRgDKXv1r"
      },
      "id": "1mHZRgDKXv1r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load documents and split them into chunks for conversion to embeddings"
      ],
      "metadata": {
        "id": "tfkgVeTzNv1x"
      },
      "id": "tfkgVeTzNv1x"
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\n",
        "    \"https://ashwinrachha.github.io/home/\",\n",
        "    \"https://medium.com/@ashwin_rachha/addition-and-subtraction-using-recurrent-neural-networks-3bb0d0b2cb86\",\n",
        "    \"https://medium.com/@ashwin_rachha/music-genre-prediction-using-ml-and-optuna-f08b4a25bbe\",\n",
        "    \"https://medium.com/@ashwin_rachha/patent-phrase-to-phrase-matching-with-pytroch-lightning-79a5f37332fa\",\n",
        "    \"https://medium.com/@ashwin_rachha/querying-a-code-database-to-find-similar-coding-problems-using-langchain-814730da6e6d\",\n",
        "    \"https://medium.com/@ashwin_rachha/deploying-a-slack-bot-with-huggingface-transformers-6962ec0fba44\",\n",
        "    \"https://medium.com/@ashwin_rachha/text-summarization-with-a-grpc-based-python-server-and-go-based-client-3bbe9badd936\"\n",
        "]"
      ],
      "metadata": {
        "id": "WmkV3Va2EbjN"
      },
      "id": "WmkV3Va2EbjN",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Loaders for Different Sources\n",
        "webloader = WebBaseLoader(urls)\n",
        "#blog_loader = RecursiveUrlLoader(url = \"https://ashwinrachha.github.io/blogpost/\", max_depth = 100, extractor=lambda x: Soup(x, \"html.parser\").text)\n",
        "text_loader = TextLoader(\"/content/ashwin.txt\")"
      ],
      "metadata": {
        "id": "HhX0M9Dp60jF"
      },
      "id": "HhX0M9Dp60jF",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pages and convert them into text and split into chunks\n",
        "pages = []\n",
        "for loader in [webloader, text_loader]:\n",
        "  pages.extend(loader.load_and_split())\n",
        "print (pages)"
      ],
      "metadata": {
        "id": "B5Fcd6dgK6JZ",
        "outputId": "8f279bf7-9b27-4618-a767-c94f82882d31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "B5Fcd6dgK6JZ",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content=\"Portfolio\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClose \\n\\n\\n\\nAbout\\n\\n\\n\\nEducation\\n\\n\\nResearch\\n\\n\\nProjects\\n\\n\\nCertifications\\n\\n\\nSkills\\n\\n\\nBlog\\n\\n\\n\\n\\n\\n\\nAshwin Rachha\\n\\nCS GRAD | NLP | VISION | ML \\n\\n\\nResume\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAbout Me\\n\\n\\n\\n                      Hello! My name is Ashwin, I am a prospective  Master's in Computer Science  student and will be joining  Virginia Tech  in Fall 2021. \\n\\n\\n\\n                    My primary interests, stemming from my immense love for literature, have been in studying and applying intelligent algorithms to NLP problems in order to understand language better and draw meaningful linguistic insights from text.\\n                    My other interests include applied computer vision and machine learning and I constanty try to learn the intricacies of the underlying intelligent Algorithms and their potential utility in everyday life.\\n                  \\n\\n\\n\\n\\n                    I have completed my Bachelor's degree in Computer Engineering from Pune Institute of Computer Technology with a GPA of 8.67/10.0\\n\\n\\n I love to write in general. As an intersection of my passion to write and to learn the depths of NLP, ML and CV, I write articles and blogposts on any new thing that I learn, as Richard Feynman says - 'What I cannot create, I do not understand.', I try to reinforce my knowledge by sharing it on my blogging platform. You can checkout my blog here >> Blog.\\n\\n\\n\\n\\n                      Get in touch with me on  linkedin,  or drop an email to me here.\\n                  \\n\\n\\n\\n\\n\\n\\nExperience\\n\\n\\nOutreach Corporation \\nMachine Learning Intern\\n\\n\\nResponsible for developing a template engine project to help Data Scientists and Machine Learning Engineers at Outreach to use templates to deploy any NLP model online inorder for them to avoide writing redundant boiler plate code.\\nDelivered an Online Inference Solution with a gRPC based Microservice in Golang serving NLP based models viz. BERT, ROBERTA and DISTILBERT for topic detection, question detection, action analysis and sentiment analysis.\\nWrote Python pipelines for ingesting data, preprocessing, tokenization, prediction and postprocessing of text data.\\nWrote Bash scripts to instantiate NLP model binaries in the ONNX format on the NVIDIA Triton Inference Server and packaged the inference solution as a docker image.\\nWrote a Go based microservice to be used to communicate with the inference server via gRPC requests and responses. Dockerized the microservice solution which would be later used to communicate with the inference service.\\nWrote tests for the application service as well as the inference service via CircleCI configuration files.\\nDeployed the application online via Kubernetes manifests on Outreach Staging Environment.\\nReduced Data Scientist efficiency time from 3-4 days to 2 Hours.\\n\\nSkills: Backend Engineering · Microservices · Online Inference · Docker · Kubernetes · CircleCI · Continuous Integration and Continuous Delivery (CI/CD) · Software Development · MLOps · Go (Programming Language) · Python (Programming Language) · Machine Learning · Deep Learning · Natural Language Processing (NLP)\\n                \\n\\n\\nMindbowser Infosolutions Pvt. Ltd. \\nDeep Learning Intern\", metadata={'source': 'https://ashwinrachha.github.io/home/', 'title': 'Portfolio', 'language': 'en'}), Document(page_content='Mindbowser Infosolutions Pvt. Ltd. \\nDeep Learning Intern\\n\\n\\nResponsible for implementing a Facial Expression Recognition (FER) application.\\nPerformed Exploratory Data Analysis on the underlying data - (FER 2013 dataset) and tested classical Machine Learning models viz. Logistic Regression, Support Vector Machine as a baseline classifier.\\nImplemented a Proof of Concept Convolutional Neural Network VGG-19 transfer learning model as the final classifier in Pytorch achieving an accuracy of 73% on the validation set.\\nIntegrated the application with a MongoDB database to store meeting metadata (timestamps, faces detected, expressions classified etc) and respective images in a GridFS format.\\nWrote a GUI script to translate the POC into a desktop application using Python Tkinter. \\nAlternatively wrote a Flask application to build a web application on the underlying model and dockerized the application. \\nPackaged the code in a python based executable which could be instantiated with a button click on the desktop as an application. \\nOwned the application from design, development to production.\\nThe project is in beta testing at Volkswagen and Bajaj India. \\n\\nSkills: Flask · Graphical User Interface (GUI) · Tkinter · OpenCV · MongoDB · Docker · Python (Programming Language) · Deep Learning · PyTorch · Keras · Scikit-Learn\\n                \\n\\n\\n\\n\\n\\nEducation\\n\\n\\n\\nPune Institute of Computer Technology\\nAug 2016 - Nov 2020GPA: 8.67/10.0\\n\\nBachelor of Engineering in Computer Engineering\\n\\n             Following are some of the courses that I took in my undergrad.\\n         \\n\\n\\nMachine Learning\\nAritifical Intelligence\\nData Analytics\\nData Mining and Warehousing\\nHigh Performance Computing\\nDistributed Systems\\nAdvanced Data Structures and Algorithms\\nObject Oriented Programming\\nTheory of Computation\\nProgramming in Java\\nOperating Systems and Systems Programming\\nComputer Organization and Architecture\\n\\n\\n\\n\\n\\n\\n\\nResearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDetecting Insincere Questions from Text: A Transfer Learning Approach\\n\\n                          The internet today has become an unrivalled source of information where people converse on content based websites such as Quora, Reddit, StackOverflow and Twitter asking doubts and sharing knowledge with the world. A major arising problem with such websites is the proliferation of toxic comments or instances of insincerity wherein the users instead of maintaining a sincere motive indulge in spreading toxic and divisive content. The....\\n                        \\nPaper\\n                        \\u2003\\n                         View on Github\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCovid 19 Chest Xray Detection : A Transfer Learning Approach\\n\\n                            The coronavirus outbreak scaused a devastating effect on people all around the world and has infected millions. The exponential escalation of the spread of the disease makes it emergent for appropriate screening methods to detect the disease and take steps in mitigating it. In this study models are developed to provide accurate diagnostics for multiclass classification (Covid vs No Findings vs Pneumonia).\\nPaper\\n                        \\u2003\\n                         View on Github\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProjects\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAutomatic Text Summarization with Pytorch and Transformers\\n\\n                          Fine tuned a T5ForConditionalGeneration (t5-base) Model on a custom News Summary dataset to perform Abstractive Text Summarization.\\n                          Created a custom dataloader from data comprising of a news core text and its human generated summary. Kept a constant mini-batch size of 2 and trained the model for 3 epochs.\\n                          The gradients were calculated using the AdamW optimizer. The training was done using Pytorch Lightning library.\\n\\n                        \\n                        \\u2003\\n                         View on Github\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRoberta for Covid-19 Tweet Sentiment Classification', metadata={'source': 'https://ashwinrachha.github.io/home/', 'title': 'Portfolio', 'language': 'en'}), Document(page_content=\"View on Github\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRoberta for Covid-19 Tweet Sentiment Classification\\n\\n                          Widening the perspective on the state of the global pandemic by harnessing the power of Twitter data and the powerful Roberta model by Facebook.\\n                          Custom cleaned tweets were fed to the Roberta tokenizer to tokenize according to the model's configuration and then a custom RobertaModel was concatenated\\n                          with two linear layers to perform sentiment analysis on the tweet data.\\n                        \\n                        \\u2003\\n                         View on Github\\n                        \\u2003\\n                         View on Kaggle\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Generating Text with BART \\n\\n                          Teaching the denoising autoencoder - BART to generate given text with Pytorch Lightning.\\n                        \\n                        \\u2003\\n                         View on Github\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLGBM Classifier + Optuna for Tabular data classification\\n\\n                          Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning. It features an imperative, define-by-run style user API.\\n                          In this project I used the new integration API of optuna to automatically tune LGBM parameters for tabular data classification.\\n                        \\n                        \\u2003\\n                         View on Github\\n                        \\u2003\\n                         View on Kaggle\\n\\n\\n\\n\\n\\n\\n\\n\\nEfficientNetb4 for Leaf disease classification\\n\\n                           At least 80% of household farms in Sub-Saharan Africa are affected by viral diseases and are major sources\\n                           of poor yields. As part of this Kaggle Competition I fine tuned an EfficientNetB4 model using fastai as well as keras\\n                           to classify images comprising of four classes of diseased leaf variants and a normal class.\\n                        \\n                        \\u2003\\n                         View on Github\\n                        \\u2003\\n                         View on Kaggle\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnsemble Regression for Tabular data.\\n\\n                          An advanced ensemble regression of Xgboost regressor + lightgbm regressor + neural network. Initially the predictions\\n                          were made using xgboost and lgbm seperately, then these predictions were averaged and fed to a neural network to make the final predictions.\\n                          This approach put me in top 10% in the competition -  Tabular Playground series - Jan 2021.\\n\\n                        \\n                        \\u2003\\n                         View on Github\\n                        \\u2003\\n                         View on Kaggle\\n\\n\\n\\n\\n\\n\\n\\n\\nGenerating Digits with Variational AutoEncoder\\n\\n\\n                          Generating Digits using Variational AutoEncoders in Keras. Inspired by Francois Chollet's tutorial in his book -  Deep Learning with Python.\\n\\n                        \\n                        \\u2003\\n                         View on Github\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNeural Style Transfer\\n\\n\\n                          In this kernel I implemented the style transfer method that is outlined in the paper, Image Style Transfer Using Convolutional Neural Networks, by Gatys in PyTorch.\\n\\nIn this paper, style transfer uses the features found in the 19-layer VGG Network, which is comprised of a series of convolutional and pooling layers, and a few fully-connected layers. In the image below, the convolutional layers are named by stack and their order in the stack.\\n                        \\n                        \\u2003\\n                         View on Github\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nText Regression using transformers and Pytorch\", metadata={'source': 'https://ashwinrachha.github.io/home/', 'title': 'Portfolio', 'language': 'en'}), Document(page_content='Text Regression using transformers and Pytorch\\n\\n                          In this recently launched competition, we are supposed to build algorithms to rate the complexity of reading passages for grade 3-12 classroom use.\\n                          I created a simple Roberta-large baseline using Root mean squared loss as its loss function to get a continuous output in native Pytorch.\\n\\n                        \\n                        \\u2003\\n                         View on Github\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nText Classification with ULMFIT fastai.\\n\\n                          ULMFit is an effective transfer learning method that can be applied to any task in NLP, and introduces techniques that are key for fine-tuning a language model.\\n                          It matches the performance of training from scratch on 100x more data.\\n                        \\n\\n                        \\u2003\\n                         View on Kaggle\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCertifications\\n\\n\\n\\n\\n\\n\\n\\n\\nDeep Learning Specialization\\n\\n\\n                              Completed the five course specialization which is a part of the famous Stanford course CS230 - Deep Learning taught by Prof. Andrew Ng.\\n                              Learnt how to train neural networks from scratch along with the calculus and linear algebra needed to do so.\\n                              Following this, learnt how to tune hyperparamters and various methods to imporve neural networks. This was followed by an in depth\\n                              learning of two most important sub-classes of modern Neural Nets i.e Convolutional Neural Networks in course 4 and Recurrent Neural Networks in course 5\\n\\n                          \\n View Certification\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNatural language processing Specialization\\n\\n                                Completed an Intensive 4 course specialization offered by deepLearning.ai that covered basic to advanced Natural Language Processing.\\n                                The four courses in this specialization include :\\n                                1.  Classification and Vector Spaces in NLP\\n                                2. Probabilistic Models in NLP\\n                                3. Sequence Models in NLP\\n                                4. Attention Models in NLP\\n                            \\n View Certification\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Python for everybody Specialization   \\n\\n                                This Specialization introduces programming concepts including data structures, networked application program interfaces, and databases, using the Python programming language. In the Capstone Project, I used the technologies learned throughout the Specialization to design and create my own  application for data retrieval, processing, and visualization.\\n                            \\n View Certification\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n     TensorFlow Developer Specialization\\n\\n                              In this 4 course specialization by deeplearning.ai I learnt how to build and train neural networks using TensorFlow in the first course. In the second course, I learnt how to improve my network’s performance using convolutions as I trained it to identify real-world images. Next up I learnt how to algorithms to understand, analyze, and respond to human speech with natural language processing systems. Process text, represent sentences as vectors, and train a model to create original poetry!\\n                            \\n View Certification\\n\\n\\n\\n\\n\\n\\nAchievements, Extracurriculars and Articles.\\n\\n\\n1. Kaggle Notebooks Expert (#1077/171,000+)\\n\\n               Currently under top 1% out of 171,000+ total Kagglers\\n               in Notebooks Category.\\n            \\n\\n\\n\\n\\n\\nSkills\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPytorch Lightning\\nScikit Learn\\nOptuna\\nXgBoost\\nLightGBM\\nFastai\\nTransformers\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                     Ashwin Rachha', metadata={'source': 'https://ashwinrachha.github.io/home/', 'title': 'Portfolio', 'language': 'en'}), Document(page_content='Addition and Subtraction using Recurrent Neural Networks. | by Ashwin Rachha | MediumAddition and Subtraction using Recurrent Neural Networks.Ashwin Rachha¬∑Follow11 min read¬∑Jan 23, 2022--ListenSharePhoto by Luca Bravo on UnsplashHow does google understand how to translate ‚Äò‰ªäÊó•„ÅØ„Å©„ÅÜ„Åß„Åô„ÅãÔºü‚Äô to ‚ÄòHow are you doing today?‚Äô or vice versa? How do we get to predict a disease spread such as COVID-19 way into the future beforehand? How do automatic Text generation or Text Summarization mechanisms work? The answer is Recurrent Neural Networks. RNNs have been the solution to deal with most problems in Natural language Processing and not only NLP but in Bio-informatics, Financial Forecasting, Sequence modelling etc. To this date, RNNs along with its variants GRUs and LSTMs are used in various state-of-the-art systems for their efficiency in modelling sequential data. In this post we will take a glimpse at how RNNs work in the background by making our RNN model learn how to automatically evaluate a mathematical equation by learning how to add or subtract numbers without using any mathematical operators ‚Äú+‚Äù or ‚Äú-‚Äù but simply by the magical properties of RNNs.What is a Recurrent Neural Network?In a traditional feed-forward neural network we can only used a fixed sized input data in which the information moves only in one direction i.e from the input layer to the hidden layer and from there to the output layer. One flavor of feed forward neural network which accepts a fixed input size is the Convolutional Neural Network which is very good in capturing spatially dependent features for eg. Nose features in a face or cat whiskers in an image of a cat. While CNNs and Feed forward networks are very good in capturing spatial features, there seldom is a scope for such data to be seen out there in the wild. Most data in the nature is of a sequential nature. Consider audio signals or textual data or Time series data. This kind of data is variable in size and hence normal feed-forward networks fail to capture the essence of such data as they do not deal with past memory. In order to capture the essence of the entire sequence of data we need to have a mechanism that captures the essence of the previous data and reinforces the contextual representation as we move along the sequence.Hence come RNNs to the rescue.Overview of feed forward network vs recurrent neural network. Credits : [ 4 ]In a RNN the information cycles through a loop. When it makes a decision, it considers the current input and also what it has learned from the inputs it received previously.RNN cell computation. Credits [ 4 ]Similar to other forms of neural networks, RNN models need to be trained in order to produce accurate and desired outputs after a set of inputs are passed through. During training, for each piece of training data we‚Äôll have a corresponding ground-truth label, or simply put a‚Äúcorrect answer‚Äù that we want the model to output. Of course, for the first few times that we pass the input data through the model, we won‚Äôt obtain outputs that are equal to these correct answers. However, after receiving these outputs, what we‚Äôll do during training is that we‚Äôll calculate the loss of that process, which measures how far off the model‚Äôs output is from the correct answer. Using this loss, we can calculate the gradient of the loss function for back-propagation.With the gradient that we just obtained, we can update the weights in the model accordingly so that future computations with the input data will produce more accurate results. The weight here refers to the weight matrices that are multiplied with the input data and hidden states during the forward pass. This entire process of calculating the gradients and updating the weights is called back-propagation. Combined with the forward pass, back-propagation is looped over and again, allowing the model to become more accurate with its outputs each time as the weight matrices values are', metadata={'source': 'https://medium.com/@ashwin_rachha/addition-and-subtraction-using-recurrent-neural-networks-3bb0d0b2cb86', 'title': 'Addition and Subtraction using Recurrent Neural Networks. | by Ashwin Rachha | Medium', 'description': 'How does google understand how to translate ‚Äò‰ªäÊó•„ÅØ„Å©„ÅÜ„Åß„Åô„ÅãÔºü‚Äô to ‚ÄòHow are you doing today?‚Äô or vice versa? How do we get to predict a disease spread such as COVID-19 way into the future beforehand? How do‚Ä¶', 'language': 'en'}), Document(page_content=\"back-propagation. Combined with the forward pass, back-propagation is looped over and again, allowing the model to become more accurate with its outputs each time as the weight matrices values are modified to pick out the patterns of the data.Although it may look as if each RNN cell is using a different weight as shown in the graphics, all of the weights are actually the same as that RNN cell is essentially being re-used throughout the process. Therefore, only the input data and hidden state carried forward are unique at each time step.Now we‚Äôve had enough theory, lets get our hands dirty!You can find the complete code here at this linkThe flow of implementation will be the following.Flow of Implementation.We shall start by importing the necessary libraries into our system.import numpy as npimport torchimport torch.nn as nnimport torch.optim as optimfrom termcolor import coloredprint(f'Using Pytorch Version - {torch.__version__}')Next up we will synthetically generate sequences of expressions using random numbers generated from 1‚Äì100 and use it to train our model and evaluate it.We define the set of all characters that can be potentially fed to our model. These include the numbers 0‚Äì9 and the operators ‚Äò+‚Äô and ‚Äò-‚Äô. Note that all characters will be input as a string and the output label will also be a string.Textual data to numeric data.Unlike humans, Neural Networks do not do very well with textual input data. In order to leverage the mathematical power of Neural Networks we have to convert them into a numerical format or a numerical encoding that quantifies a particular character or maps a character to a numerical format. There are various formats in which this numerical mapping can be done ‚Äî Bag of words representation, Tf-idf vectors, One hot vectors and the newest and most effective method i.e Word Embeddings. Since the scale of our problems is not large and the vocabulary size is small we can make do with One hot vector representations to encode our data.We will use a character_to_index dictionary to get the index at which the character resides and an index_to_character dictionary which maps an index to the encoded character. We will use these dictionaries alternatively while encoding and decoding our input sequence.all_chars = '0123456789+-'num_features = len(all_chars)char_to_index = {c : i for i, c in enumerate(all_chars)}index_to_char = {i : c for i, c in enumerate(all_chars)}print(f'Number of features : {len(all_chars)}')We will use the np.random.randint function which is a Pseudo Random Number Generator to generator two number between 0 and 100 and a equal probability distribution of addition and subtraction symbols. We will store the expression as a string and evaluate the expressions and save the results of the evaluations in a string called labels.def generate_data():    first_num = np.random.randint(low=0,high=100)    second_num = np.random.randint(low=0,high=100)    add = np.squeeze(np.random.randint(low=0, high=100)) > 50.    if add:      example = str(first_num) + '+' + str(second_num)      label = str(first_num+second_num    else:      example = str(first_num) + '-' + str(second_num)      label = str(first_num-second_num)   return example, labelgenerate_data()Output :('74+10', '84')Next we will generate batches of inputs and target variables that will be fed to our RNN model and for that we need to define empty placeholders. Since we are restricting our input sequences to length 5 i.e double digits ‚Äú+/-‚Äù followed by another double digits and the output can also be fit in 5 characters, the maximum time steps of our input sequence can be fixed as 5.We represent each token as a more expressive feature vector. The easiest representation for representing a character is one-hot encoding. In this form of representation we take row vectors of size corresponding to the vocabulary size of the characters and mark those indexes as 1 where that character is present in the vocabulary. For example if\", metadata={'source': 'https://medium.com/@ashwin_rachha/addition-and-subtraction-using-recurrent-neural-networks-3bb0d0b2cb86', 'title': 'Addition and Subtraction using Recurrent Neural Networks. | by Ashwin Rachha | Medium', 'description': 'How does google understand how to translate ‚Äò‰ªäÊó•„ÅØ„Å©„ÅÜ„Åß„Åô„ÅãÔºü‚Äô to ‚ÄòHow are you doing today?‚Äô or vice versa? How do we get to predict a disease spread such as COVID-19 way into the future beforehand? How do‚Ä¶', 'language': 'en'}), Document(page_content=\"form of representation we take row vectors of size corresponding to the vocabulary size of the characters and mark those indexes as 1 where that character is present in the vocabulary. For example if our input word is ‚ÄòGOOD‚Äô, the vocabulary size becomes 3 (Set of unique characters) and if the input dictionary is defined as {G : 0, O : 1, D : 2}, we can represent G as [1, 0, 0], O as [0, 1, 0] and D as [0, 0, 1] hence the representation for GOOD will be denoted as[[1,0,0], [0,1,0] ,[0,1,0], [0,0,1]]We will use the encode function to convert our example expressions into their one hot encoded versions.def encode(example, label): x = np.zeros((max_time_steps, num_features)) y = np.zeros((max_time_steps, num_features)) diff_x = max_time_steps - len(example)  diff_y = max_time_steps - len(label)  for i, c in enumerate(example):   x[diff_x+i, char_to_index[c]] = 1   for i in range(diff_x):      x[i, char_to_index['0']] = 1   for i, c in enumerate(label):     y[diff_y+i, char_to_index[c]] = 1    for i in range(diff_y):      y[i, char_to_index['0']] = 1return x, ySimilarly we will use the decode function to restore our outputs back to the format we intend them to be in. At the end of our Computation we will be left with a vector storing output probabilities of the shape [max_time_steps, num_features] which will store the output evaluation. We take the argmax from each of the row vectors and take the index with the highest probability and convert it into the output expression using our dictionary int_to_char that we previously computed.def decode(example): res = [index_to_char[np.argmax(vec)] for i, vec in enumerate(example)] return ''.join(res)def strip_zeros(example): encountered_non_zero = False output = '' for c in example:   if not encountered_non_zero and c == '0':      continue   if c == '+' or c == '-':     encountered_non_zero = False   else: encountered_non_zero = True output += c return outputGenerating Batches and Converting to Tensors.Here we will call the generate function to generate batches of examples and labels and store them in our placeholders.def create_dataset(num_examples=20000): x_train = np.zeros((num_examples, max_time_steps, num_features)) y_train = np.zeros((num_examples, max_time_steps, num_features)) for i in range(num_examples):   e, l = generate_data()   x, y = encode(e, l)   x_train[i] = x   y_train[i] = y return x_train, y_trainx_train, y_train = create_dataset(200000)We will generate about 20000 and convert the numpy arrays into Pytorch tensors as tensors are the fundamental functional units in Pytorch. Pytorch and numpy work hand in hand with each other.x_train = torch.from_numpy(x_train)y_train = torch.from_numpy(y_train)x_train = torch.tensor(x_train, dtype = torch.float32)y_train = torch.tensor(y_train, dtype = torch.float32)Working with neural nets is very labor intensive when done on a CPU since it consists of a large number of matrix multiplications which cries out for parallelism. Thankfully with the advent of modern hardware called GPUs these vast amounts of parallel matrix multiplications can be done very efficiently on multiple cores. In fact one of the major reasons for the boom of Deep Learning has been associated with the advancements of GPUs.We will set our device type to cuda if we have any GPUs available then Pytorch will automatically use the GPU else it will be set to the default device i.e CPU.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')Our Simple RNN model.Now we come to cream of the project ‚Äî defining the model for our task!The nn.Module acts as a Base class for all Neural Network architectures in Pytorch. We will inherit this class to extend our SimpleRNN class. In the constructor of our class we can define some variables that our model will be using such as the hidden dimension size, the number of layers in the RNN cell etc. For our model we will be using one RNN layer followed by a fully connected layer that outputs one single variable at each\", metadata={'source': 'https://medium.com/@ashwin_rachha/addition-and-subtraction-using-recurrent-neural-networks-3bb0d0b2cb86', 'title': 'Addition and Subtraction using Recurrent Neural Networks. | by Ashwin Rachha | Medium', 'description': 'How does google understand how to translate ‚Äò‰ªäÊó•„ÅØ„Å©„ÅÜ„Åß„Åô„ÅãÔºü‚Äô to ‚ÄòHow are you doing today?‚Äô or vice versa? How do we get to predict a disease spread such as COVID-19 way into the future beforehand? How do‚Ä¶', 'language': 'en'}), Document(page_content='such as the hidden dimension size, the number of layers in the RNN cell etc. For our model we will be using one RNN layer followed by a fully connected layer that outputs one single variable at each timestep which shall be passed over a softargmax layer to get the index of the prediction.We also need to pass an initial hidden state to our RNN cell for initial computation which is initialized to zeros for the first computation.class SimpleRNN(nn.Module):  def __init__(self, input_size, output_size, hidden_dim,n_layers):   super(SimpleRNN, self).__init__()   self.hidden_dim = hidden_dim   self.n_layers = n_layers   self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first = True)   self.fc1 = nn.Linear(hidden_dim,hidden_dim * 2)   self.fc2 = nn.Linear(hidden_dim * 2, output_size)   self.relu = nn.ReLU()  def forward(self, x):   batch_size = x.size(0)   hidden = self.init_hidden(batch_size)   hidden = hidden.cuda()   out, hidden = self.rnn(x, hidden)   out = self.fc1(out)   out = self.fc2(self.relu(out))   return out, hidden def init_hidden(self, batch_size):  hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)  return hiddenInitializing the model and defining hyperparameters.After defining the model above, we‚Äôll have to instantiate the model with the relevant parameters and define our hyper-parameters as well. The hyper-parameters we‚Äôre defining below are:n_epochs : Number of Epochs --> Number of times our model will go through the entire training datasetlr : Learning Rate --> Rate at which our model updates the weights in the cells each time back-propagation is doneFor a more in-depth guide on hyper-parameters, you can refer to this comprehensive article.Similar to other neural networks, we have to define the optimizer and loss function as well. We‚Äôll be using CrossEntropyLoss as the final output is basically a classification task and the common Adam optimizer.model = SimpleRNN(input_size = num_features, output_size = num_features , hidden_dim = 12, n_layers = 1)model.cuda()n_epochs = 1000lr = 0.01criterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(), lr = lr)Training the model.Now we can begin our training! As we only have a few examples, the training process is very fast. However, as we progress, larger datasets and deeper models mean that the input data is much larger and the number of parameters within the model that we have to compute is much more.This training process involves two processes one is called the Forward Propagation and the other is the Backward Propagation. In the forward propagation the model multiplies the input with the previous hidden state for each time step to finally generate the final hidden state. During the Backward Propagation step the gradients are sent backward by calculating the loss with a given criterion (in this case ‚Äî the cross entropy loss) and the weights of the model are updated so that the model can make more accurate predictions.for epoch in range(1, n_epochs + 1): optimizer.zero_grad() x_train = x_train.cuda() output, hidden = model(x_train) loss = criterion(output, y_train.cuda()) loss.backward() optimizer.step() if epoch % 10 == 0:   print(\\'Epoch: {}/{}.............\\'.format(epoch, n_epochs), end=\\' \\')   print(\"Loss: {:.4f}\".format(loss.item()))Evaluation.Let‚Äôs test our model now and see what kind of output we will get. As a first step, we‚Äôll define some helper function to convert our model output back to text.full_seq_acc = 0for i, pred in enumerate(preds): pred_str = strip_zeros(decode(pred)) y_test_str = strip_zeros(decode(y_test[i])) x_test_str = strip_zeros(decode(x_test[i])) col = \\'green\\' if pred_str == y_test_str else \\'red\\' full_seq_acc += 1/len(preds) * int(pred_str == y_test_str) outstring = \\'Input: {}, Out: {}, Pred: {}\\'.format(x_test_str, y_test_str, pred_str) print(colored(outstring, col)) print(\\'\\\\nFull sequence accuracy: {:.3f} %\\'.format(100 * full_seq_acc))And Voila! We can see a hundred percent accurate results', metadata={'source': 'https://medium.com/@ashwin_rachha/addition-and-subtraction-using-recurrent-neural-networks-3bb0d0b2cb86', 'title': 'Addition and Subtraction using Recurrent Neural Networks. | by Ashwin Rachha | Medium', 'description': 'How does google understand how to translate ‚Äò‰ªäÊó•„ÅØ„Å©„ÅÜ„Åß„Åô„ÅãÔºü‚Äô to ‚ÄòHow are you doing today?‚Äô or vice versa? How do we get to predict a disease spread such as COVID-19 way into the future beforehand? How do‚Ä¶', 'language': 'en'}), Document(page_content=\"y_test_str, pred_str) print(colored(outstring, col)) print('\\\\nFull sequence accuracy: {:.3f} %'.format(100 * full_seq_acc))And Voila! We can see a hundred percent accurate results while testing. Thus we have seen the power of Recurrent Neural Networks in sequence modelling. The function of RNNs can be extrapolated to a number of other tasks such as Text Summarization, Text Generation, Time Series Forecasting and much more. Today there are better variants of RNNs namely Gated Recurrent Units and Long Short Term Memory networks which have little tweaks in their architecture to solve two main problems with vanilla RNNs i.e 1. Vanishing and Exploding Gradients and 2. Inability to capture long term sequence dependency. Today with the introduction of the Transformer Architecture even LSTMs and GRUs are outperformed by some of the state of the art variants which make use of the attention mechanism and completely eliminate the use of recurrence. But the good old RNN still remains in use today and has laid a foundation for some of the legendary works in NLP.References :[1] http://web.stanford.edu/class/cs224n/slides/cs224n-2022-lecture06-fancy-rnn.pdf[2] http://karpathy.github.io/2015/05/21/rnn-effectiveness/[3] http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/[4] https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/[5] https://blog.paperspace.com/recurrent-neural-networks-part-1-2/[6] https://arxiv.org/pdf/1506.00019.pdfArtificial IntelligenceMachine LearningNatural Language ProcessRecurrent Neural NetworkPython----FollowWritten by Ashwin Rachha18 FollowersFollowMore from Ashwin RachhaAshwin RachhaQuerying a Code Database to find Similar Coding Problems using Langchain.The Dawn of the Planet of Language Models8 min read¬∑Apr 24--2Ashwin RachhaPatent Phrase-to-Phrase Matching with Pytorch LightningKaggle Recently Launched an NLP based competition wherein competitors are required to extract meaning from a large text-based dataset for‚Ä¶9 min read¬∑May 11, 2022--Ashwin RachhaDeploying a Slack bot with HuggingFace Transformers \\uf8ffü§óSlack is a widely used team collaboration platform that enables users to communicate with their teammates, share files, and integrate‚Ä¶5 min read¬∑May 8--Ashwin RachhaText Summarization with a gRPC based Python Server and Go based Client.So you have tried and tested your Deep Learning model and it seems to work absolutely fine on your Jupyter notebook environment. That is‚Ä¶7 min read¬∑Sep 2, 2022--1See all from Ashwin RachhaRecommended from MediumKrishnakanth Naik JarapalaCrash Course in Time Series Analysis and Forecastingby Krishnakanth Naik Jarapala, Venkata Bhargavi Sikhakolli13 min read¬∑Apr 10--Prajjwal ChauhanStock Prediction and Forecasting Using LSTM(Long-Short-Term-Memory)In an ever-evolving world of finance, accurately predicting stock market movements has long been an elusive goal for investors and traders‚Ä¶6 min read¬∑Jul 8--4ListsPredictive Modeling w/ Python20 stories¬∑316 savesPractical Guides to Machine Learning10 stories¬∑347 savesNatural Language Processing551 stories¬∑175 savesChatGPT21 stories¬∑124 savesRohaankForecasting: Predicting Future Prices using LSTM ArchitectureIn the world of real estate, predicting house prices accurately can be a challenging task. However, with the advancements in machine‚Ä¶8 min read¬∑Jul 8--Dominik PolzerinTowards Data ScienceAll You Need to Know to Build Your First LLM AppA step-by-step tutorial to document loaders, embeddings, vector stores and prompt templates¬∑26 min read¬∑Jun 22--42TeeTrackerFine-tuning LLMsTasks to finetune6 min read¬∑Jul 22--1Pedro FuentesSentiment Analysis with RNNs and BERTDecoding Emotions in Text with Sentiment Analysis12 min read¬∑Aug 20--See more recommendationsHelpStatusWritersBlogCareersPrivacyTermsAboutText to speechTeams\", metadata={'source': 'https://medium.com/@ashwin_rachha/addition-and-subtraction-using-recurrent-neural-networks-3bb0d0b2cb86', 'title': 'Addition and Subtraction using Recurrent Neural Networks. | by Ashwin Rachha | Medium', 'description': 'How does google understand how to translate ‚Äò‰ªäÊó•„ÅØ„Å©„ÅÜ„Åß„Åô„ÅãÔºü‚Äô to ‚ÄòHow are you doing today?‚Äô or vice versa? How do we get to predict a disease spread such as COVID-19 way into the future beforehand? How do‚Ä¶', 'language': 'en'}), Document(page_content=\"Music Genre Prediction using ML and Optuna | by Ashwin Rachha | MediumMusic Genre Prediction using ML and OptunaAshwin Rachha¬∑Follow10 min read¬∑Feb 19, 2022--ListenShareMusic is everywhere around us. The impact of music on human beings cannot be expressed in words. Through its perennial journey, the face of music has seen a lot of diversity and change in its form. The world around us is not homogenous and there exists a vast diversity in the lifestyle of people, their thinking and their culture. With this diversity it is apparent that music too will be different and will have diversity that matches to the interests of different people. Keeping this diversity in mind there is tremendous scope for audio streaming and media services companies to automatically classify the genre of a particular song in order to cater similar songs to potential listeners. With the current boom in Data Science practices and predictive modelling and with the availability of structured/unstructured data online such tasks have become convenient and are at the forefront of large companies.In this online project we shall aim to categorize songs into their respective categories viz. (Electronic, Rap, Hip-Hop, Pop) based on some certain parameters or features given in a tabular data format on the basis of which we classify which category the songs belong to.The first and foremost task would be to import all the necessary libraries.# Standard Data Manipulation Librariesimport pandas as pdimport numpy as npfrom pathlib import Pathimport ospath = Path('../input/prediction-of-music-genre')‚Äã# Profile Report and Statistical analysis from pandas_profiling import profile_report‚Äã# Preprocessing Data Librariesfrom sklearn.preprocessing import LabelEncoderfrom sklearn.impute import SimpleImputerfrom sklearn.preprocessing import label_binarizefrom sklearn.preprocessing import StandardScaler‚Äã# Data Splitting Librariesfrom sklearn.model_selection import train_test_splitfrom sklearn.model_selection import StratifiedKFold, KFold‚Äã# Performance Metrics Librariesfrom sklearn.metrics import confusion_matrix, accuracy_scorefrom sklearn.metrics import roc_curve, classification_report‚Äã# Modelsfrom sklearn.linear_model import LogisticRegressionfrom sklearn.naive_bayes import GaussianNBfrom xgboost import XGBClassifierfrom lightgbm import LGBMClassifierfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.svm import SVCfrom mlxtend.classifier import StackingCVClassifier‚Äã# Plottingimport plotly.express as pximport plotly.graph_objects as gofrom plotly.offline import plot, iplot, init_notebook_modeimport matplotlib.pyplot as pltimport seaborn as snssns.set_style('darkgrid')  # sets the background of plots to a darkgridplt.rcParams['figure.figsize'] = [12.0, 8.0] # Defaults the size of the figures to specified valueinit_notebook_mode(connected=True)dataframe = pd.read_csv(path / 'music_genre.csv')The core idea of machine learning is ‚Äî based on some particular numericalized input, through statistical/mathematical approximation techniques we learn a mapping of this input data to a specified output value. After learning how to represent the features of the model on the training data ‚Äî annotated (X, Y) pairs we then calculate the efficiency of the model on newly seen test data that is a simulated representative of data in the real word. For the model to accurately model the behaviour of the input data, it is absolutely essential for the input data to be clean, consistent and valid.So Lets take a look at the given data at hand.print(f'Number of Columns in the Dataset : {len(dataframe.columns)}')col_df = {}for col in dataframe.columns:    col_df[col] = dataframe[col].dtypepd.DataFrame(col_df.items(), columns = ['Column Name', 'Data Type']).head(18)The columns of the dataset consists of 18 fields which are representative of a particular song in the dataset. The\", metadata={'source': 'https://medium.com/@ashwin_rachha/music-genre-prediction-using-ml-and-optuna-f08b4a25bbe', 'title': 'Music Genre Prediction using ML and Optuna | by Ashwin Rachha | Medium', 'description': 'Music is everywhere around us. The impact of music on human beings cannot be expressed in words. Through its perennial journey, the face of music has seen a lot of diversity and change in its form‚Ä¶', 'language': 'en'}), Document(page_content='columns = [\\'Column Name\\', \\'Data Type\\']).head(18)The columns of the dataset consists of 18 fields which are representative of a particular song in the dataset. The columns are as follows :\\\\instance_id : Serial number of the song in the dataset.artist_name : Name of the artist of the song.track_name : Title of the song.popularity : An arbitrary score assigned to the song in the range of 0‚Äì100 with 100 being most popular and 0 being least.acousticness : This value describes how acoustic a song is. A score of 1.0 means the song is most likely to be an acoustic one.danceability : Danceability describes how suitable a track is for dancing based on a combination of musical elements. A value of 0.0 is least danceable and 1.0 is most danceableduration_ms : Is the duration in milliseconds of the song.energy : Represents how energetic the song is. The range of this field is between [0‚Äì1] with 1 being song with highest energy and 0 with lowest.instrumentalness : This value represents the amount of vocals in the song. The closer it is to 1.0, the more instrumental the song iskey : Key of a piece is the group of pitches, or scale, that forms the basis of a music composition.liveness : This value describes the probability that the song was recorded with a live audience.loudness : Column representing how loud the song is.mode : Major and Minor scales that the song is based upon.speechiness : Speechiness detects the presence of spoken words in a track.tempo : Speed at which the song is being played.obtained_date : The date at which the song metadata was retrieved.valence : A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive.music_genre : The actual category to which the song belongs. This is our target variable.dataframe.head()Let us Visualize how many songs are there in each genre. We will calculate the value count of each genre and plot them using plotly. Note that there are 5 missing values or ‚Äònan‚Äô values in the dataframe so we exclude them while plotting.genre_names = [col for col in dataframe[\\'music_genre\\'].unique() if type(col)!=float]colors = [    \\'#1f77b4\\',  # muted blue    \\'#ff7f0e\\',  # safety orange    \\'#2ca02c\\',  # cooked asparagus green    \\'#d62728\\',  # brick red    \\'#9467bd\\',  # muted purple    \\'#8c564b\\',  # chestnut brown    \\'#e377c2\\',  # raspberry yogurt pink    \\'#7f7f7f\\',  # middle gray    \\'#bcbd22\\',  # curry yellow-green    \\'#17becf\\'   # blue-teal]fig = px.bar(x=genre_names,              y=dataframe[\\'music_genre\\'].value_counts().values,             labels={\\'x\\': \\'Genre\\', \\'y\\': \\'Value Counts\\'},             color = colors,            title=\\'Count of Songs belonging to each Genre\\')fig.show()So we can observe that the dataset is pretty balanced meaning there are equal number of samples in each category which makes our job kind of easier. An imbalanced dataset is a problematic one in that one class which is comparatively present in a greater quantity gets more represented compared to other classes which leads to bias towards that particular task. In such cases accuracy becomes an inaccurate measure of performance of the model since the probability of landing onto one class is not equal so even a random classifier would do well on the dataset (i.e show high accuracy).fig = px.histogram(dataframe, x=\"popularity\", nbins = 20, title = \\'Distribution of Popularity Values\\')fig.update_layout(bargap = 0.2)fig.show()We can see the distribution of popularity values for the songs. The mode of the histogram points to the range 50‚Äì54 which states that most songs lie in that area of popularity. The songs with high/ extremely high popularity value ‚Äî [80‚Äì100] are quite low, which makes sense as there are only a few songs which make it to the top billboard charts. This values of this particular column are symmetrical around the median hence this normally distributed column can be a very important feature in determining the genre of a particular song.fig =', metadata={'source': 'https://medium.com/@ashwin_rachha/music-genre-prediction-using-ml-and-optuna-f08b4a25bbe', 'title': 'Music Genre Prediction using ML and Optuna | by Ashwin Rachha | Medium', 'description': 'Music is everywhere around us. The impact of music on human beings cannot be expressed in words. Through its perennial journey, the face of music has seen a lot of diversity and change in its form‚Ä¶', 'language': 'en'}), Document(page_content='This values of this particular column are symmetrical around the median hence this normally distributed column can be a very important feature in determining the genre of a particular song.fig = px.histogram(dataframe, x=\"danceability\", nbins = 20, title = \\'Distribution of Danceability Values\\', color_discrete_sequence = [\\'darksalmon\\'])fig.update_layout(bargap = 0.2)fig.show()Similar to popularity the danceability is also a symmetric distribution along the median which can be a potentially good indicator to classify genre‚Äôs. The intuition behind this column could be interpreted as ‚Äî some songs are calm and hence have low danceability value compared to some songs which are party heavy songs which have high danceability value.fig = px.histogram(dataframe, x=\"energy\", nbins = 20, title = \\'Distribution of Energy Values\\', color_discrete_sequence = [\\'seagreen\\'])fig.update_layout(bargap = 0.2)fig.show()fig = px.histogram(dataframe, x=\"acousticness\", nbins = 20, title = \\'Distribution of Acousticness Values\\', color_discrete_sequence=[\\'indianred\\'])fig.update_layout(bargap = 0.2)fig.show()fig = px.histogram(dataframe, x=\"instrumentalness\", nbins = 20, title = \\'Distribution of Instrumentalness Values\\', color_discrete_sequence=[\\'darkturquoise\\'])fig.update_layout(bargap = 0.2)fig.show()fig = px.histogram(dataframe, x=\"liveness\", nbins = 20, title = \\'Distribution of Acousticness Values\\', color_discrete_sequence=[\\'purple\\'])fig.update_layout(bargap = 0.2)fig.show()key_names = [col for col in dataframe[\\'key\\'].unique() if type(col)!=float]fig = px.bar(x=key_names,              y=dataframe[\\'key\\'].value_counts().values,             labels={\\'x\\': \\'Key\\', \\'y\\': \\'Value Counts\\'},             color = colors + [\\'purple\\', \\'orange\\'] ,            title=\\'Count of Key types\\')fig.show()dataframe.isnull().sum()n = len(dataframe)for col in dataframe.columns:    missing = sum(dataframe[col].isnull())    print(f\\'Percentage of Missing values in {col} column : {(missing / n) * 100} %\\')The dataset is a good quality dataset with almost negligible missing values in each column. Still we shall drop the columns with missing values inorder to make sure our models do not face problems for nan values while training.dataframe = dataframe.dropna()dataframe.isnull().sum()duplicatedRows = dataframe[dataframe.duplicated()]print(\\'Duplicated Rows in the Dataset are :\\')print(len(duplicatedRows))The Dataset also does not contain any duplicate values that have to be taken care of manually.profile = dataframe.profile_report()profileprofile.to_file(output_file=\"Dataset_Profile_Report.html\")dataframe.info()After this brief EDA, it is time to create models for prediction. For the feature engineering part we have to take care of the following things -Dropping Columns that do not or might not contribute much in prediction.Dealing with Missing values or Nan values and imputing them with mean for continuous variables and with mode for categorical variablesdataframe.head()train = dataframe.drop(columns = [\\'instance_id\\', \\'artist_name\\', \\'track_name\\', \\'obtained_date\\'])continuous_columns = [col for col in train.columns if dataframe[col].dtype == float]categorical_columns = [col for col in train.columns if dataframe[col].dtype == object]continuous_columns, categorical_columnsimpute_sum = 0for val in train[\\'tempo\\']:    if val!= \\'?\\':        impute_sum += float(val)impute_val = impute_sum // len(train[\\'tempo\\'])Some values from the ‚Äòtempo‚Äô class had wrongly included a ‚Äò?‚Äô token hence making the column‚Äôs datatype as object. We can impute and replace the ‚Äò?‚Äô tags with the mean of all other values in the column.train[\\'tempo\\'].replace(to_replace = \\'?\\', value = impute_val, inplace=True)train[\\'tempo\\'] = train[\\'tempo\\'].astype(float)Some values from the ‚Äòduration_ms‚Äô column had wrongly included a ‚Äò-1‚Äô token. This is a wrong interpretation since if a song title and artist name exists then it needs to have duration in time which is not negative (which is impossible). We do a similar', metadata={'source': 'https://medium.com/@ashwin_rachha/music-genre-prediction-using-ml-and-optuna-f08b4a25bbe', 'title': 'Music Genre Prediction using ML and Optuna | by Ashwin Rachha | Medium', 'description': 'Music is everywhere around us. The impact of music on human beings cannot be expressed in words. Through its perennial journey, the face of music has seen a lot of diversity and change in its form‚Ä¶', 'language': 'en'}), Document(page_content='a ‚Äò-1‚Äô token. This is a wrong interpretation since if a song title and artist name exists then it needs to have duration in time which is not negative (which is impossible). We do a similar imputation for this field.train[\\'duration_ms\\'].replace(to_replace = -1, value = train[\\'duration_ms\\'].mean(), inplace=True)There are a few categorical variable in our input features as well as our target variable is a categorical feature represented by strings. In order to feed these into our model we need to convert them into an encoded representation. We will use the LabelEncoder class from sklearn which encodes target labels with value between 0 and n_classes-1.le = LabelEncoder()for col in categorical_columns:    train[col] = le.fit_transform(train[col])    print(le.classes_)train.columns# Training with all columns includedX = train.drop(columns = [\\'music_genre\\'])Y = train[\\'music_genre\\'].values‚Äãtemp_X = train.drop(columns = [\\'tempo\\', \\'duration_ms\\', \\'music_genre\\'])temp_Y = train[\\'music_genre\\'].valuesfrom sklearn.model_selection import train_test_splitxtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size = 0.2, random_state = 786)for ds in [xtrain, xtest, ytrain, ytest]:    print(f\\'Shape : {ds.shape}\\')model1 = \\'Logistic Regression\\'lr = LogisticRegression(solver=\\'liblinear\\')model = lr.fit(xtrain, ytrain)ypred = lr.predict(xtest)lr_cm = confusion_matrix(ytest, ypred)lr_acc = accuracy_score(ytest, ypred)print(\\'Confusion Matrix\\')print(lr_cm)print(\\'\\\\n\\')print(f\\'Accuracy of {model1} : {lr_acc *100} \\\\n\\')print(classification_report(ytest, ypred))from sklearn.model_selection import train_test_splitxtrain, xtest, ytrain, ytest = train_test_split(temp_X, temp_Y, test_size = 0.2, random_state = 786)model1 = \\'Logistic Regression\\'lr = LogisticRegression(solver=\\'liblinear\\')model = lr.fit(xtrain, ytrain)ypred = lr.predict(xtest)lr_cm = confusion_matrix(ytest, ypred)lr_acc = accuracy_score(ytest, ypred)print(\\'Confusion Matrix\\')print(lr_cm)print(\\'\\\\n\\')print(f\\'Accuracy of {model1} : {lr_acc *100} \\\\n\\')print(classification_report(ytest, ypred))model2 = \\'Naive Bayes\\'nb = GaussianNB()nb.fit(xtrain, ytrain)ypred = nb.predict(xtest)nb_cm = confusion_matrix(ytest, ypred)nb_acc = accuracy_score(ytest, ypred)print(\\'Confusion Matrix\\')print(nb_cm)print(\\'\\\\n\\')print(f\\'Accuracy of {model2} : {nb_acc * 100} \\\\n\\')print(classification_report(ytest, ypred))model3 = \\'Random Forest Classifer\\'rf = RandomForestClassifier(n_estimators = 20, random_state = 2, max_depth = 5)rf.fit(xtrain,ytrain)ypred = rf.predict(xtest)rf_cm = confusion_matrix(ytest, ypred)rf_acc = accuracy_score(ytest, ypred)print(\"confussion matrix\")print(rf_cm)print(\"\\\\n\")print(f\"Accuracy of {model3} : {rf_acc*100}\\\\n\")print(classification_report(ytest,ypred))model4 = \\'K Neighbors Classifier\\'knn = KNeighborsClassifier(n_neighbors = 10)knn.fit(xtrain, ytrain)ypred = knn.predict(xtest)knn_cm = confusion_matrix(ytest, ypred)knn_acc = accuracy_score(ytest, ypred)print(\\'Confusion Matrix\\')print(knn_cm)print(\\'\\\\n\\')print(f\\'Accuracy of {model4} : {knn_acc * 100} \\\\n\\')print(classification_report(ytest, ypred))model5 = \\'DecisionTreeClassifier\\'dt = DecisionTreeClassifier(criterion = \\'entropy\\', random_state = 0, max_depth = 6)dt.fit(xtrain, ytrain)ypred = dt.predict(xtest)dt_cm = confusion_matrix(ytest, ypred)dt_acc = accuracy_score(ytest, ypred)print(\\'Confusion Matrix\\')print(dt_cm)print(\\'\\\\n\\')print(f\\'Accuracy of {model5} : {dt_acc * 100} \\\\n\\')print(classification_report(ytest, ypred))model6 = \\'Support Vector Classifier\\'svc = SVC(kernel = \\'rbf\\', C = 2)svc.fit(xtrain, ytrain)ypred = svc.predict(xtest)svc_cm = confusion_matrix(ytest, ypred)svc_acc = accuracy_score(ytest, ypred)print(\\'Confusion Matrix\\')print(svc_cm)print(\\'\\\\n\\')print(f\\'Accuracy of {model6} : {svc_acc * 100} \\\\n\\')print(classification_report(ytest, ypred))model7 = \\'Extreme Gradient Boosting\\'xgb = XGBClassifier()xgb.fit(xtrain, ytrain)ypred = xgb.predict(xtest)xgb_cm = confusion_matrix(ytest, ypred)xgb_acc = accuracy_score(ytest, ypred)print(\\'Confusion', metadata={'source': 'https://medium.com/@ashwin_rachha/music-genre-prediction-using-ml-and-optuna-f08b4a25bbe', 'title': 'Music Genre Prediction using ML and Optuna | by Ashwin Rachha | Medium', 'description': 'Music is everywhere around us. The impact of music on human beings cannot be expressed in words. Through its perennial journey, the face of music has seen a lot of diversity and change in its form‚Ä¶', 'language': 'en'}), Document(page_content='= \\'Extreme Gradient Boosting\\'xgb = XGBClassifier()xgb.fit(xtrain, ytrain)ypred = xgb.predict(xtest)xgb_cm = confusion_matrix(ytest, ypred)xgb_acc = accuracy_score(ytest, ypred)print(\\'Confusion Matrix\\')print(xgb_cm)print(\\'\\\\n\\')print(f\\'Accuracy of {model7} : {xgb_acc * 100} \\\\n\\')print(classification_report(ytest, ypred))model8 = \\'Light Gradient Boosting Machine\\'lgb = LGBMClassifier()lgb.fit(xtrain, ytrain)ypred = lgb.predict(xtest)lgb_cm = confusion_matrix(ytest, ypred)lgb_acc = accuracy_score(ytest, ypred)print(\\'Confusion Matrix\\')print(lgb_cm)print(\\'\\\\n\\')print(f\\'Accuracy of {model7} : {lgb_acc * 100} \\\\n\\')print(classification_report(ytest, ypred))from sklearn.feature_selection import RFEselector = RFE(lgb, n_features_to_select=1)selector.fit(xtrain, ytrain)print(f\"Model\\'s Feature Importance\")for i in range(len(selector.ranking_)):    print(f\"#{i+1}: {temp_X.columns[selector.ranking_[i]-1]} \")from sklearn.model_selection import cross_val_scoreimport warningswarnings.filterwarnings(\\'ignore\\')Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.kf =KFold(n_splits=10, shuffle=True, random_state=24)‚Äãaccuracy_logr = cross_val_score(lr, xtrain, ytrain, scoring=\"accuracy\", cv=kf)accuracy_dtree = cross_val_score(xgb, xtrain, ytrain, scoring=\"accuracy\", cv=kf)accuracy_nb = cross_val_score(lgb, xtrain, ytrain, scoring=\"accuracy\", cv=kf)‚Äãaccuracy_logr = accuracy_logr.mean()accuracy_dtree = accuracy_dtree.mean()accuracy_nb = accuracy_nb.mean()‚Äãprint(\\'k-fold Cross Validation Accuracy Outputs:\\')print(\\'-\\')print(\"Logistic Regression:\", round(accuracy_logr*100,2),\"%\")print(\"Xtreme Gradient Boosting:\", round(accuracy_dtree*100,2),\"%\")print(\"Light GBM:\", round(accuracy_nb*100,2),\"%\")Using Optuna to automatically tune the hyperparameters of the LGBM model.import optuna.integration.lightgbm as lgbdtrain = lgb.Dataset(xtrain, label = ytrain)dtest = lgb.Dataset(xtest, label = ytest)params = {\\'objective\\' : \\'multiclass\\', \\'num_class\\' : 10,  \\'metric\\' : \\'multi_logloss\\', \\'verbosity\\' : -1, \\'boosting_type\\' : \\'gbdt\\'}model = lgb.train(params, dtrain, valid_sets = [dtest], verbose_eval = 100, early_stopping_rounds = 100)params = model.paramsparamsfrom lightgbm import LGBMClassifiermodel = LGBMClassifier(**params)model.fit(xtrain, ytrain, eval_set = ((xtest, ytest)), early_stopping_rounds = 50, verbose = 0)ypred = model.predict(xtest)lgb_cm = confusion_matrix(ytest, ypred)lgb_acc = accuracy_score(ytest, ypred)print(\\'Confusion Matrix\\')print(lgb_cm)print(\\'\\\\n\\')print(f\\'Accuracy of {model7} : {lgb_acc * 100} \\\\n\\')print(classification_report(ytest, ypred))Thank you very much for following through. As a budding blogger it means a lot that you have taken the efforts to glide through the entire page, doesnt matter if you liked it or not. I am grateful.Checkout the entire notebook on my Kaggle profile with visualizations output at this link:Music Genre PredictionReach out to me at:LinkedinKaggle.Machine LearningOptunaXgboostMachine Learning PythonMachine Learning Ai----FollowWritten by Ashwin Rachha18 FollowersFollowMore from Ashwin RachhaAshwin RachhaQuerying a Code Database to find Similar Coding Problems using Langchain.The Dawn of the Planet of Language Models8 min read¬∑Apr 24--2Ashwin RachhaPatent Phrase-to-Phrase Matching with Pytorch LightningKaggle Recently Launched an NLP based competition wherein competitors are required to extract meaning from a large text-based dataset for‚Ä¶9 min read¬∑May 11, 2022--Ashwin RachhaAddition and Subtraction using Recurrent Neural Networks.How does google understand how to translate ‚Äò‰ªäÊó•„ÅØ„Å©„ÅÜ„Åß„Åô„ÅãÔºü‚Äô to ‚ÄòHow are you doing today?‚Äô or vice versa? How do we get to predict a disease‚Ä¶11 min read¬∑Jan 23, 2022--Ashwin RachhaDeploying a Slack bot with', metadata={'source': 'https://medium.com/@ashwin_rachha/music-genre-prediction-using-ml-and-optuna-f08b4a25bbe', 'title': 'Music Genre Prediction using ML and Optuna | by Ashwin Rachha | Medium', 'description': 'Music is everywhere around us. The impact of music on human beings cannot be expressed in words. Through its perennial journey, the face of music has seen a lot of diversity and change in its form‚Ä¶', 'language': 'en'}), Document(page_content='to translate ‚Äò‰ªäÊó•„ÅØ„Å©„ÅÜ„Åß„Åô„ÅãÔºü‚Äô to ‚ÄòHow are you doing today?‚Äô or vice versa? How do we get to predict a disease‚Ä¶11 min read¬∑Jan 23, 2022--Ashwin RachhaDeploying a Slack bot with HuggingFace Transformers \\uf8ffü§óSlack is a widely used team collaboration platform that enables users to communicate with their teammates, share files, and integrate‚Ä¶5 min read¬∑May 8--See all from Ashwin RachhaRecommended from MediumAlidu AbubakariBuilding a Sales Prediction App with Streamlit and Machine LearningI. Introduction12 min read¬∑Apr 9--Prashun chauhaninFasal EngineeringStreamlit: Whip up a Web App In MinutesStreamlit is an open-source Python library that isused to make data apps without any knowledge of frontend service. The best part is if you‚Ä¶6 min read¬∑Mar 7--1ListsPractical Guides to Machine Learning10 stories¬∑347 savesPredictive Modeling w/ Python20 stories¬∑316 savesNatural Language Processing551 stories¬∑175 savesThe New Chatbots: ChatGPT, Bard, and Beyond13 stories¬∑96 savesRizki Aji MahardikaDiabetes Prediction Using StreamlitBusiness Problem', metadata={'source': 'https://medium.com/@ashwin_rachha/music-genre-prediction-using-ml-and-optuna-f08b4a25bbe', 'title': 'Music Genre Prediction using ML and Optuna | by Ashwin Rachha | Medium', 'description': 'Music is everywhere around us. The impact of music on human beings cannot be expressed in words. Through its perennial journey, the face of music has seen a lot of diversity and change in its form‚Ä¶', 'language': 'en'}), Document(page_content='A hospital wanted to improve the quality of patient care by identifying the potential risk of diabetes in patients‚Ä¶5 min read¬∑6 days ago--Paolino IorioAnomaly Detection and Clustering: a PCA and t-SNE approach6 min read¬∑Aug 18--Krishnakanth Naik JarapalaCrash Course in Time Series Analysis and Forecastingby Krishnakanth Naik Jarapala, Venkata Bhargavi Sikhakolli13 min read¬∑Apr 10--Alex Saruni LodaruCustomer Churn Predictions Using Classification AnalysisChurn refers to the number of customers who stop using a product or service over a given period of time. Customers churn for various‚Ä¶13 min read¬∑Jun 4--See more recommendationsHelpStatusWritersBlogCareersPrivacyTermsAboutText to speechTeams', metadata={'source': 'https://medium.com/@ashwin_rachha/music-genre-prediction-using-ml-and-optuna-f08b4a25bbe', 'title': 'Music Genre Prediction using ML and Optuna | by Ashwin Rachha | Medium', 'description': 'Music is everywhere around us. The impact of music on human beings cannot be expressed in words. Through its perennial journey, the face of music has seen a lot of diversity and change in its form‚Ä¶', 'language': 'en'}), Document(page_content='Patent Phrase-to-Phrase Matching with Pytorch Lightning | by Ashwin Rachha | MediumPatent Phrase-to-Phrase Matching with Pytorch LightningAshwin Rachha¬∑Follow9 min read¬∑May 11, 2022--ListenShareKaggle Recently Launched an NLP based competition wherein competitors are required to extract meaning from a large text-based dataset for patents. One cannot get a patent if the invention is already available out there or has been publicly disclosed. Therefore in Research and while registering for innovations and scientific patents it is absolutely important for a semantic search to be performed to confirm whether the invention that one wants to patent has already been registered or not.It is because of this reason the U.S Patent and Trademark Office offers one of the largest repositories of scientific, technical and commercial information in the world through its Open Data Portal.‚ÄúIt‚Äôs been nearly impossible to unlock this valuable data effectively in the past, but by leveraging emerging technologies such as big data and machine learning, we are able to better serve our customers‚Äù ‚Äî USPTO Chief Data Strategist Thomas A. Beach.In this project we shall leverage the power of state-of-the-art transformer models which have known to perform astoundingly well to solve the problem of semantic similarity search and extract relevant information by matching key phrases in patent documents. Determining the semantic similarity between phrases is critically important during the patent search and examination process to determine if an invention has been described before.The patent domain is prepared for being exploited by tranformer based models to reap value to various businesses and save billions of dollars yearly. Patents hold the potential for tremendous business values since many companies are on the run for innovation and registering their novels works, patents account for billions of dollars of revenue in transacting the rights and developing them also reviewing patent applications.This competition and further research should help a broader community in the application of Machine Learning in the patent domain, particularly in semantic search.Bert For PatentsGoogle used a Large BERT model (Bidirectional Encoder Representations from Transformers) for data from patent officies across the United States and many other countries. This study illuminated on how transformers could be leveraged to understand the context of patents in spite of the synonymous nature of the keywords or tokenzed words. BERT is smart enough to weigh the same context term differently in the sample abstracts.To train the model, Google used a Large BERT training implantation using the core open-sourced Python libraries with the following hyperparameters trained on an 8√ó8 TPU slice on GCP. Of the over 2,000 terms that the USPTO provides as example synonyms, ~200 exist in multiple CPC codes. These synonyms that exist across multiple CPC codes provide a good mechanism to test how well the BERT algorithm is able to generate different synonyms for the same term in different contexts.According to Google, this is how BERT approaches patent applications:Select a CPC code.Select a term.Query ‚ÄôN‚Äô number of patent documents containing the term within the given CPC code.Generate predictions for each term for each document.Calculate aggregate metrics reflecting the highest predicted terms on average across all ‚ÄôN‚Äô documents.Pytorch LightningWe shall use Pytorch Lightning for fine-tuning our Bert-For-Patents model on the data provided for the competition. Pytorch Lightning is optimized for research. In a fast paced modular environment of designing AI systems and models, a lot of unnecessary time is waster in ironing out errors and reusing redundant code for training and evaluating the deep learning pipelines. Not to mention thinking about scaling the applications to be run on multiple GPUs in the world of Big Data where datasets are getting Bigger and Bigger. Unlike Native', metadata={'source': 'https://medium.com/@ashwin_rachha/patent-phrase-to-phrase-matching-with-pytroch-lightning-79a5f37332fa', 'title': 'Patent Phrase-to-Phrase Matching with Pytorch Lightning | by Ashwin Rachha | Medium', 'description': 'Kaggle Recently Launched an NLP based competition wherein competitors are required to extract meaning from a large text-based dataset for patents. One cannot get a patent if the invention is already‚Ä¶', 'language': 'en'}), Document(page_content=\"the deep learning pipelines. Not to mention thinking about scaling the applications to be run on multiple GPUs in the world of Big Data where datasets are getting Bigger and Bigger. Unlike Native Pytorch, Pytorch Lightning is a high-level framework that abstracts away implementation details for traning, validating, testing and logging of deep learning models so the entire focus could be placed on the two most important aspects of Deep Learning ‚Äî Data and Models. Developers can focusing on preparing precise data points with actionable insights and design models apt to fit those data points to extract as much value from the datapoints and transform them aptly to our tasks.Source : https://www.assemblyai.com/blog/pytorch-lightning-for-dummies/Source : https://www.assemblyai.com/blog/pytorch-lightning-for-dummies/We will be needing the following libraries to make our lives easier -import pandas as pdimport numpy as npimport torchimport torch.nn as nnfrom torchmetrics import MeanSquaredErrorfrom torch.utils.data import Dataset, DataLoaderfrom transformers import AutoModel, AutoTokenizerfrom sklearn.model_selection import train_test_splitimport pytorch_lightning as plfrom pytorch_lightning import Trainer, seed_everythingfrom pytorch_lightning import Callbackfrom pytorch_lightning.loggers import CSVLoggerfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStoppingfrom pathlib import Pathimport ospath = (Path('../input/us-patent-phrase-to-phrase-matching'))We will use Pathlib to hardcode the path in to a PosixPath object that will make it easier to deal with absolute file paths.os.listdir(path)['sample_submission.csv', 'train.csv', 'test.csv']train = pd.read_csv(path / 'train.csv')test = pd.read_csv(path / 'test.csv')sub = pd.read_csv(path / 'sample_submission.csv')train.head()idanchortargetcontextscore037d61fd2272659b1abatementabatement of pollutionA470.5017b9652b17b68b7a4abatementact of abatingA470.75236d72442aefd8232abatementactive catalystA470.2535296b0c19e1ce60eabatementeliminating processA470.50454c1e3b9184cb5b6abatementforest regionA470.00Dataset Info \\uf8ffüìàColumns of the train data-id - a unique identifier for a pair of phrasesanchor - the first phrasetarget - the second phrasescore - the similarity. This is sourced from a combination of one or more manual expert ratingsHere are some plots related to the Exploratory Data Analysis.Distribution of ScoresDistribution of Anchor Text Word CountDistribution of Target Text Word CountThe first letter is the ‚Äúsection symbol‚Äù consisting of a letter from ‚ÄúA‚Äù (‚ÄúHuman Necessities‚Äù) to ‚ÄúH‚Äù (‚ÄúElectricity‚Äù) or ‚ÄúY‚Äù for emerging cross-sectional technologies. This is followed by a two-digit number to give a ‚Äúclass symbol‚Äù (‚ÄúA01‚Äù represents ‚ÄúAgriculture; forestry; animal husbandry; trapping; fishing‚Äù).A: Human NecessitiesB: Operations and TransportC: Chemistry and MetallurgyD: TextilesE: Fixed ConstructionsF: Mechanical EngineeringG: PhysicsH: ElectricityY: Emerging Cross-Sectional TechnologiesWe will declare some variables that will be required to initialize model and training parameters.Configurationclass config:    val_size = 0.2    max_len = 128    model_name = '../input/bert-for-patents/bert-for-patents'    batch_size = 16    epochs = 1    lr = 2e-5    max_lr = 1e-3    steps_per_epoch = None    accumulate = 1    patience = 3    monitor = 'val_loss'    seed = 42    debug = FalseDataset and DataLoaderPytorch provides two data primitives : Dataset and DataLoader that allow users to use preloaded datasets as well as their own datasets. Dataset stores the samples and their corresponding labels. In this case the init function takes the tokenizer and the dataframe as inputs to the constructor as well as a string phase signifying which dataset we are dealing with train or test.We can now create a tokenizer for this model. Note that pretrained models assume that text is tokenized in a particular way. In order to ensure that your tokenizer matches your\", metadata={'source': 'https://medium.com/@ashwin_rachha/patent-phrase-to-phrase-matching-with-pytroch-lightning-79a5f37332fa', 'title': 'Patent Phrase-to-Phrase Matching with Pytorch Lightning | by Ashwin Rachha | Medium', 'description': 'Kaggle Recently Launched an NLP based competition wherein competitors are required to extract meaning from a large text-based dataset for patents. One cannot get a patent if the invention is already‚Ä¶', 'language': 'en'}), Document(page_content=\"with train or test.We can now create a tokenizer for this model. Note that pretrained models assume that text is tokenized in a particular way. In order to ensure that your tokenizer matches your model, use the AutoTokenizer, passing in your model name.We‚Äôll need to combine the context, anchor, and target together somehow. There‚Äôs not much research as to the best way to do this, so we may need to iterate a bit. To start with, we‚Äôll just combine them all into a single string. The model will need to know where each section starts, so we can use the special separator token to tell it:class SimilarityDataset(Dataset):        def __init__(self, dataframe, tokenizer, phase = 'train'):        self.dataframe = dataframe        self.tokenizer = tokenizer        self.phase = phase    def __len__(self):        return len(self.dataframe)        def __getitem__(self, index):        anchor = self.dataframe.anchor.iloc[index].lower()        target = self.dataframe.target.iloc[index].lower()                text = anchor + '[SEP]' + target + '[SEP]'                tokens = self.tokenizer(            text,            truncation = True,             padding = 'max_length',            max_length = config.max_len        )        if self.phase == 'train':            score = torch.tensor(self.dataframe.score.iloc[index], dtype = torch.float32)            return (            np.array(tokens['input_ids']),            np.array(tokens['attention_mask']),            score            )        else:            return(                np.array(tokens['input_ids']),                np.array(tokens['attention_mask'])            )DataLoader wraps an iterable around the Dataset to enable easy access to the samples. Each iteration below returns a batch of train_features and train_labels (containing batch_size=64 features and labels respectively). Because we specified shuffle=True, after we iterate over all batches the data is shuffled (for finer-grained control over the data loading order, take a look at Samplers).scores = train.score.valuestrain.drop('score', inplace = True, axis = 1)train_data, val_data, train_labels, val_labels = train_test_split(train, scores, stratify = scores, test_size = config.val_size, random_state = config.seed)train_data['score'] = train_labelsval_data['score'] = val_labels‚Äãtokenizer = AutoTokenizer.from_pretrained(config.model_name)train_dataset = SimilarityDataset(train_data, tokenizer, phase = 'train')val_dataset = SimilarityDataset(val_data, tokenizer, phase = 'train')test_dataset = SimilarityDataset(test, tokenizer, phase = 'test')‚Äãtrain_dataloader = DataLoader(train_dataset, batch_size = config.batch_size, shuffle = True)val_dataloader = DataLoader(val_dataset, batch_size = config.batch_size, shuffle = False)test_dataloader = DataLoader(test_dataset, batch_size = config.batch_size, shuffle = False)Source : https://arxiv.org/abs/1810.04805Now we will Load the pretrained Bert-for-Patents model using the nn.Module base class provided by Pytorch. Modules can also contain other Modules, allowing to nest them in a tree structure. Additionally we pass the encodings of the final layer of bert to a linear layer which produces one continuous variable as this problem is modelled as a regression problem.class SimilarityModel(nn.Module):    def __init__(self, model_name):        super().__init__()        self.model = AutoModel.from_pretrained(model_name)        self.head = nn.Linear(1024, 1, bias = True)        self.dropout = nn.Dropout(0.5)        def forward(self, input_ids, attention_mask):        x = self.model(input_ids, attention_mask)        x = torch.sum(x[0], 1)/ x[0].shape[1]        x = self.dropout(x)        x = self.head(x)        return xWe then encapsulte our model in the pl.Lightningmodule base class. The structure of the class is self-explanatory, that is, in addition to defining the neural network and the forward function, with PyTorch Lightning we can define what we want to be done in each batch execution as well as\", metadata={'source': 'https://medium.com/@ashwin_rachha/patent-phrase-to-phrase-matching-with-pytroch-lightning-79a5f37332fa', 'title': 'Patent Phrase-to-Phrase Matching with Pytorch Lightning | by Ashwin Rachha | Medium', 'description': 'Kaggle Recently Launched an NLP based competition wherein competitors are required to extract meaning from a large text-based dataset for patents. One cannot get a patent if the invention is already‚Ä¶', 'language': 'en'}), Document(page_content='is self-explanatory, that is, in addition to defining the neural network and the forward function, with PyTorch Lightning we can define what we want to be done in each batch execution as well as in each epoch for both the training and validation data, on the other hand, we also observe that the optimizer is isolated, which allows us to have a better organization of each element of the training phase.class PLModel(pl.LightningModule):    def __init__(self, model, criterion, metric):        super(PLModel, self).__init__()        self.model = model        self.criterion = criterion        self.metric = metric            def forward(self, input_ids, attention_mask):        return self.model(input_ids, attention_mask)        def configure_optimizers(self):        self.optimizer = torch.optim.Adam(self.model.parameters(), lr = config.lr)        return self.optimizer        def training_step(self, batch, batch_idx):        ids, mask = batch[0], batch[1]        preds = self.model(ids, mask)        loss = self.criterion(preds.squeeze(1), batch[2])        rmse = self.metric(preds.squeeze(1), batch[2])        logs = {            \\'train_loss\\' : loss,            \\'train_error\\' : rmse,            \\'lr\\' : self.optimizer.param_groups[0][\\'lr\\']        }        self.log_dict(logs, on_step = False, on_epoch = True, prog_bar = True, logger = True)        return loss        def validation_step(self, batch, batch_idx):        ids, mask = batch[0],  batch[1]        preds = self.model(ids, mask)        loss = self.criterion(preds.squeeze(1), batch[2])        rmse = self.metric(preds.squeeze(1), batch[2])        logs = {            \\'val_loss\\' : loss,            \\'val_error\\' : rmse        }        self.log_dict(logs, on_step = False, on_epoch = True, prog_bar = True, logger = True)        return loss        def predict_step(self, batch, batch_idx):        ids, mask = batch[0], batch[1]        preds = self.model(ids, mask)        return predsFor the metrics calculation, you can just make use of the built-in metric functions that Lightning provides.The logs are saved ‚Äúautomatically‚Äù, that is, you do not have to use a specific library, it only makes use of the ‚Äúself.log‚Äù variable that comes from the ‚Äúpl.LightningModule‚Äù extension, your logs will be saved automatically.Training!In Pytorch Lightning training can be done with the Trainer method provided which provides an abstraction for training.This abstraction achieves the following:You maintain control over all aspects via PyTorch code without an added abstraction.The trainer allows overriding any key part that you don‚Äôt want automated.Under the hood, the Lightning Trainer handles the training loop details for you, some examples include:Automatically enabling/disabling gradsRunning the training, validation and test dataloadersCalling the Callbacks at the appropriate timesPutting batches and computations on the correct deviceslogger = CSVLogger(save_dir=\\'./\\', name=config.model_name.split(\\'/\\')[-1]+\\'_log\\')logger.log_hyperparams(config.__dict__)checkpoint_callback = ModelCheckpoint(monitor=config.monitor,                                      save_top_k=1,                                      save_last=True,                                      save_weights_only=True,                                      filename=\\'{epoch:02d}-{valid_loss:.4f}-{valid_acc:.4f}\\',                                      verbose=False,                                      mode=\\'min\\')early_stop_callback = EarlyStopping(monitor=config.monitor,                                     patience=config.patience,                                     verbose=False,                                     mode=\"min\")‚Äãtrainer = Trainer(    max_epochs=config.epochs,    gpus=[0],    accumulate_grad_batches=config.accumulate,    callbacks=[checkpoint_callback, early_stop_callback],     logger=logger,    weights_summary=\\'top\\',)Putting it all together and training!model = SimilarityModel(config.model_name)criterion =', metadata={'source': 'https://medium.com/@ashwin_rachha/patent-phrase-to-phrase-matching-with-pytroch-lightning-79a5f37332fa', 'title': 'Patent Phrase-to-Phrase Matching with Pytorch Lightning | by Ashwin Rachha | Medium', 'description': 'Kaggle Recently Launched an NLP based competition wherein competitors are required to extract meaning from a large text-based dataset for patents. One cannot get a patent if the invention is already‚Ä¶', 'language': 'en'}), Document(page_content=\"callbacks=[checkpoint_callback, early_stop_callback],     logger=logger,    weights_summary='top',)Putting it all together and training!model = SimilarityModel(config.model_name)criterion = nn.HuberLoss(reduction='mean', delta=1.0)metric = MeanSquaredError()driver = PLModel(model, criterion, metric)‚Äãtrainer.fit(driver, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)Post in Progress‚Ä¶Please Upvote!Pytorch LightningKaggleBertNLPNaturallanguageprocessing----FollowWritten by Ashwin Rachha18 FollowersFollowMore from Ashwin RachhaAshwin RachhaQuerying a Code Database to find Similar Coding Problems using Langchain.The Dawn of the Planet of Language Models8 min read¬∑Apr 24--2Ashwin RachhaAddition and Subtraction using Recurrent Neural Networks.How does google understand how to translate ‚Äò‰ªäÊó•„ÅØ„Å©„ÅÜ„Åß„Åô„ÅãÔºü‚Äô to ‚ÄòHow are you doing today?‚Äô or vice versa? How do we get to predict a disease‚Ä¶11 min read¬∑Jan 23, 2022--Ashwin RachhaDeploying a Slack bot with HuggingFace Transformers \\uf8ffü§óSlack is a widely used team collaboration platform that enables users to communicate with their teammates, share files, and integrate‚Ä¶5 min read¬∑May 8--Ashwin RachhaText Summarization with a gRPC based Python Server and Go based Client.So you have tried and tested your Deep Learning model and it seems to work absolutely fine on your Jupyter notebook environment. That is‚Ä¶7 min read¬∑Sep 2, 2022--1See all from Ashwin RachhaRecommended from MediumGanymede NilHow to Segment Large Texts for Better SummarizationText summarization is a challenging task that aims to produce concise and informative summaries of long texts. However, most existing text‚Ä¶4 min read¬∑Feb 27--1Syed HamzaWhy LLMs are consuming more memory than Call of Duty:Warzone?Understanding Memory Consumption: Why Language Models Require More Memory Than High End PC gaming4 min read¬∑Jul 1--ListsNatural Language Processing551 stories¬∑175 savesThe New Chatbots: ChatGPT, Bard, and Beyond13 stories¬∑96 savesNow in AI: Handpicked by Better Programming266 stories¬∑108 savesNew_Reading_List174 stories¬∑84 savesDominik PolzerinTowards Data ScienceAll You Need to Know to Build Your First LLM AppA step-by-step tutorial to document loaders, embeddings, vector stores and prompt templates¬∑26 min read¬∑Jun 22--42Dr.PixelinAI MindDistributed Training with PytorchThis week I needed to train a pytorch model using a cluster of 4 machines with 9 GPUs. I learnt a lot in this exercise and I am here to‚Ä¶¬∑6 min read¬∑Jul 19--Jatin TyagiInstruct Fine-Tuning Falcon 7B Using LoRAIntroduction5 min read¬∑Jul 25--TeeTrackerFine-tuning LLMsTasks to finetune6 min read¬∑Jul 22--1See more recommendationsHelpStatusWritersBlogCareersPrivacyTermsAboutText to speechTeams\", metadata={'source': 'https://medium.com/@ashwin_rachha/patent-phrase-to-phrase-matching-with-pytroch-lightning-79a5f37332fa', 'title': 'Patent Phrase-to-Phrase Matching with Pytorch Lightning | by Ashwin Rachha | Medium', 'description': 'Kaggle Recently Launched an NLP based competition wherein competitors are required to extract meaning from a large text-based dataset for patents. One cannot get a patent if the invention is already‚Ä¶', 'language': 'en'}), Document(page_content='Querying a Code Database to find Similar Coding Problems using Langchain. | by Ashwin Rachha | MediumQuerying a Code Database to find Similar Coding Problems using Langchain.Ashwin Rachha¬∑Follow8 min read¬∑Apr 24--2ListenShareThe Dawn of the Planet of Language ModelsThe world of Large Language Models has taken by storm. There has been a surge in the interest and investment of LLMs by large corporations and open source developers alike due to the advancements in Natural Language Processing, Machine Learning Engineering and Computational Resources. Large language models work by predicting the probability of a sequence of words given a context. They can perform tasks such as question answering, summarization, translation, and chatbot conversation by using different types of inputs and outputs. For example, to answer a question, the model can take the question as an input and generate the answer as an output. To summarize a text, the model can take the text as an input and generate a shorter version as an output. Large language models have revolutionized the field of natural language processing in recent years. They have demonstrated impressive abilities such as multi-step arithmetic, taking college-level exams, decoding the International Phonetic Alphabet, and generating similar English equivalents of Kiswahili proverbs. However, they also have some limitations and challenges, such as hallucination, bias, ethical issues, and environmental impact.In this post we will use Microsoft Codebert along with Langchain to output similar leetcode problems based on a query problem. The similarity matching will be based on how similar the output solutions are rather than collecting tags or keywords. In the coming posts we shall also see how LLMs can be used as a proxy between a stored Knowledge base and the end user and make the chatbot functionality seamless with amazing text based outputs.You can directly play with the application here:https://huggingface.co/spaces/PinoCorgi/DSA_RecommendorCodeBERT is a pre-trained model for programming and natural languages, which can be used for various code intelligence tasks, such as code search, code summarization, and code translation¬π. One of the applications of CodeBERT is to find similar code snippets given a query code snippet. This can be useful for code reuse, plagiarism detection, or code recommendation.To use CodeBERT for code similarity, one possible approach is to encode the query code and the candidate codes using CodeBERT, and then compute the cosine similarity between their embeddings. The higher the cosine similarity, the more similar the codes are. This approach leverages the pre-trained contextual embeddings from CodeBERT, which can capture both syntactic and semantic similarities between codes.LangChain is a Python library that provides out-of-the-box support to build NLP applications using large language models (LLMs) such as GPT-3. You can connect to various data and computation sources, and build applications that perform NLP tasks on domain-specific data sources, private repositories, and much more.Some of the features of LangChain are:Prompt engineering: LangChain helps you manage, optimize, and serialize prompts for different LLMs and tasks.Chains: LangChain allows you to create sequences of calls to LLMs or other utilities, such as APIs, databases, or indexes.Data augmented generation: LangChain enables you to fetch data from external sources and use it in the generation step, such as summarizing long texts or answering questions over specific documents.Agents: LangChain lets you create agents that can interact with their environment, make decisions, and take actions based on LLMs.Memory: LangChain supports persisting state between calls of a chain or an agent, using different memory implementations.Evaluation: LangChain provides some prompts and chains for evaluating generative models using LLMs themselves.LangChain is designed to help you create powerful and differentiated', metadata={'source': 'https://medium.com/@ashwin_rachha/querying-a-code-database-to-find-similar-coding-problems-using-langchain-814730da6e6d', 'title': 'Querying a Code Database to find Similar Coding Problems using Langchain. | by Ashwin Rachha | Medium', 'description': 'The world of Large Language Models has taken by storm. There has been a surge in the interest and investment of LLMs by large corporations and open source developers alike due to the advancements in‚Ä¶', 'language': 'en'}), Document(page_content='implementations.Evaluation: LangChain provides some prompts and chains for evaluating generative models using LLMs themselves.LangChain is designed to help you create powerful and differentiated applications that are not only calling out to an LLM via an API, but also being data-aware and agentic¬≤. Some of the common use cases that LangChain supports are:Personal assistants: LangChain can help you create personal assistants that can take actions, remember interactions, and have knowledge about your data.Question answering: LangChain can help you create question answering systems that can answer questions over specific documents, using only the information in those documents.Chatbots: LangChain can help you create chatbots that can produce natural and engaging text responses, using LLMs and other sources of information.Querying tabular data: LangChain can help you use LLMs to query data and in this blog post we will see an example of a Simple Querying of Tabular data using Codebert.!pip install -q langchain==0.0.123 openai==0.27.2 redis==4.5.3 numpy pandas sentence-transformers deeplake tiktoken faiss-cpu[2K     [90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [32m17.0/17.0 MB[0m [31m82.1 MB/s[0m eta [36m0:00:00[0m[?25h# Importsimport pandas as pdfrom bs4 import BeautifulSoupimport getpassimport osfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.document_loaders.csv_loaderimport CSVLoaderfrom langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.embeddings import HuggingFaceEmbeddingsfrom langchain.vectorstores import FAISSfrom bs4 import BeautifulSoup: This import is necessary to use the BeautifulSoup library, which is a tool for parsing and extracting information from HTML and XML documents. BeautifulSoup can be used to scrape code snippets from websites or online repositories, such as GitHub or Stack Overflow.from langchain.text_splitter import CharacterTextSplitter: This import is necessary to use the CharacterTextSplitter class from the langchain library, which is a tool for splitting text into smaller chunks based on a character limit. CharacterTextSplitter can be used to split long code snippets into shorter ones that can fit into the input of a language model, such as GPT-3.from langchain.document_loaders.csv_loader import CSVLoader: This import is necessary to use the CSVLoader class from the langchain library, which is a tool for loading documents from a CSV file into a langchain index. CSVLoader can be used to create a code database from a CSV file that contains code snippets and their metadata, such as language, source, or tags.from langchain.embeddings.openai import OpenAIEmbeddings: This import is necessary to use the OpenAIEmbeddings class from the langchain library, which is a tool for generating embeddings for text using OpenAI‚Äôs language models, such as GPT-3. OpenAIEmbeddings can be used to create vector representations of code snippets that capture their semantic and syntactic features.from langchain.embeddings import HuggingFaceEmbeddings: This import is necessary to use the HuggingFaceEmbeddings class from the langchain library, which is a tool for generating embeddings for text using Hugging Face‚Äôs language models, such as BERT or CodeBERT. HuggingFaceEmbeddings can be used to create vector representations of code snippets that capture their semantic and syntactic features.from langchain.vectorstores import FAISS: This import is necessary to use the FAISS class from the langchain library, which is a tool for storing and retrieving vectors using Facebook‚Äôs FAISS library, which is a fast and efficient similarity search engine. FAISS can be used to create an index of code embeddings that can support fast and accurate queries for similar code snippets.df = pd.read_csv(\"/content/scraped_contents.csv\").dropna(subset = \"Python\").reset_index()df.head()Next up we will load the', metadata={'source': 'https://medium.com/@ashwin_rachha/querying-a-code-database-to-find-similar-coding-problems-using-langchain-814730da6e6d', 'title': 'Querying a Code Database to find Similar Coding Problems using Langchain. | by Ashwin Rachha | Medium', 'description': 'The world of Large Language Models has taken by storm. There has been a surge in the interest and investment of LLMs by large corporations and open source developers alike due to the advancements in‚Ä¶', 'language': 'en'}), Document(page_content='that can support fast and accurate queries for similar code snippets.df = pd.read_csv(\"/content/scraped_contents.csv\").dropna(subset = \"Python\").reset_index()df.head()Next up we will load the ‚Äúscaped_contents.csv‚Äù file which was created by scraping my github repository containing solved leetcode questions and their answers. I used Beautiful Soup to scrape the repository and create a csv file. You can find the link to my Leetcode Solutions Repository here. This dataset was created by scraping leetcode python solution files (.py) , their corresponding notes and readme markdown files (.md) files. Shoutout to Qasim Wani and his extension Leethub that enables you to upload your leetcode solutions to your github repo as you as you hit the submit button.print(len(df))305EmbeddingsEmbeddings are numerical representations of text or code that capture their semantic and syntactic features. Embeddings can be used for various natural language processing and code tasks, such as search, clustering, classification, and generation12.OpenAI embeddings are embeddings that are generated by OpenAI‚Äôs language models, such as GPT-3. OpenAI embeddings measure the relatedness of text or code strings, and can be used for tasks like search, recommendations, anomaly detection, and diversity measurement3. OpenAI offers different types of embeddings for different functionalities, such as text similarity, text search, and code search3. For example, text-similarity-davinci-001 is a model that provides embeddings that capture the semantic similarity between pieces of text3.Hugging Face embeddings are embeddings that are generated by Hugging Face‚Äôs language models, such as BERT or CodeBERT. Hugging Face embeddings can be used for tasks like classification, topic modeling, and code understanding45. Hugging Face offers a variety of pre-trained models for different domains and languages that can produce embeddings4. For example, microsoft/codebert-base is a model that provides embeddings for natural language and programming language pairs6.os.environ[\\'OPENAI_API_KEY\\'] = getpass.getpass(\\'OpenAI API Key:\\')OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑loader = CSVLoader(file_path=\"/content/scraped_contents.csv\", source_column = \"Python\")data = loader.load()text_splitter = CharacterTextSplitter(\\t\\t\\tchunk_size=1000, chunk_overlap=0)docs = text_splitter.split_documents(data)This code snippet shows how to use the langchain library to load and split code snippets from a CSV file. Here is a possible explanation for each line of code:loader = CSVLoader(file_path=\"/content/scraped_contents.csv\", source_column = \"Python\"): This line creates a CSVLoader object that can load documents from a CSV file. The file_path argument specifies the path to the CSV file, and the source_column argument specifies the name of the column that contains the code snippets. In this case, the file is located at \"/content/scraped_contents.csv\" and the column name is \"Python\".data = loader.load(): This line calls the load method of the CSVLoader object, which returns a list of documents. Each document is a dictionary that contains the code snippet and its metadata, such as language, source, or tags.text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0): This line creates a CharacterTextSplitter object that can split long documents into smaller chunks based on a character limit. The chunk_size argument specifies the maximum number of characters in each chunk, and the chunk_overlap argument specifies the number of overlapping characters between adjacent chunks. In this case, the chunk size is 1000 and there is no overlap.docs = text_splitter.split_documents(data): This line calls the split_documents method of the CharacterTextSplitter object, which takes a list of documents as input and returns a list of chunks. Each chunk is a dictionary that contains the text and its metadata, such as index, offset, or length.model_name = \"mchochlov/codebert-base-cd-ft\"hf =', metadata={'source': 'https://medium.com/@ashwin_rachha/querying-a-code-database-to-find-similar-coding-problems-using-langchain-814730da6e6d', 'title': 'Querying a Code Database to find Similar Coding Problems using Langchain. | by Ashwin Rachha | Medium', 'description': 'The world of Large Language Models has taken by storm. There has been a surge in the interest and investment of LLMs by large corporations and open source developers alike due to the advancements in‚Ä¶', 'language': 'en'}), Document(page_content='documents as input and returns a list of chunks. Each chunk is a dictionary that contains the text and its metadata, such as index, offset, or length.model_name = \"mchochlov/codebert-base-cd-ft\"hf = HuggingFaceEmbeddings(model_name=model_name)embeddings = OpenAIEmbeddings()We import a pretrained CodeBert Model from Huggingface Model Repository and use the HuggingFaceEmbeddings class which allows us to use the mdoel as a sentence embedding model. It will convert the code to vector embeddings.db = FAISS.from_documents(docs, hf)We then create a FAISS vector database from the documents and their embeddings, which can be used for similarity search.def get_similar_links(query):  \\tembedding_vector = hf.embed_query(query)  docs_and_scores = db.similarity_search_by_vector(embedding_vector)  hrefs = []  for docs in docs_and_scores:    html_doc = docs.page_content    soup = BeautifulSoup(html_doc, \\'html.parser\\')    href = [a[\\'href\\'] for a in soup.find_all(\\'a\\', href=True)]    hrefs.append(href)  return hrefsAnd finally we create a simple cosine-similarity search over the entire corpus of vectors by converting our query vector into CodeBert Embeddings. And voila! We can finally use this function to get similar codes according to the context.print(f\"The Similar Questions to solve after solving {df[\\'directory\\'].iloc[5]} are \\\\\\\\n {get_similar_links(df[\\'Python\\'].iloc[5])}\")The Similar Questions to solve after solving 230-kth-smallest-element-in-a-bst are:Deepest Leaves SumPath SumPath Sum IIIMaximum Average Subtreeprint(f\"The Similar Questions to solve after solving {df[\\'directory\\'].iloc[33]} are \\\\\\\\n {get_similar_links(df[\\'Python\\'].iloc[33])}\")The Similar Questions to solve after solving 1249-minimum-remove-to-make-valid-parentheses are:Remove All Adjacent Duplicates in String IIVerify Preorder Serialization of a Binary TreeRemove K Digitsprint(f\"The Similar Questions to solve after solving {df[\\'directory\\'].iloc[44]} are \\\\\\\\n {get_similar_links(df[\\'Python\\'].iloc[44])}\")The Similar Questions to solve after solving 1512-number-of-good-pairs are:Find the DifferenceReduce Array Size to The HalfSingle Element in a Sorted ArrayReferences(1) GitHub ‚Äî microsoft/CodeBERT: CodeBERT. https://github.com/microsoft/CodeBERT.(2) CodeBERT: A Pre-Trained Model for Programming and Natural Languages. https://arxiv.org/abs/2002.08155.(3) https://python.langchain.com/en/latest/index.html(4) https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/(5) Try the app : https://huggingface.co/spaces/PinoCorgi/DSA_RecommendorPythonLarge Language ModelsLangchainRecommendationsLeetcode----2FollowWritten by Ashwin Rachha18 FollowersFollowMore from Ashwin RachhaAshwin RachhaPatent Phrase-to-Phrase Matching with Pytorch LightningKaggle Recently Launched an NLP based competition wherein competitors are required to extract meaning from a large text-based dataset for‚Ä¶9 min read¬∑May 11, 2022--Ashwin RachhaAddition and Subtraction using Recurrent Neural Networks.How does google understand how to translate ‚Äò‰ªäÊó•„ÅØ„Å©„ÅÜ„Åß„Åô„ÅãÔºü‚Äô to ‚ÄòHow are you doing today?‚Äô or vice versa? How do we get to predict a disease‚Ä¶11 min read¬∑Jan 23, 2022--Ashwin RachhaDeploying a Slack bot with HuggingFace Transformers \\uf8ffü§óSlack is a widely used team collaboration platform that enables users to communicate with their teammates, share files, and integrate‚Ä¶5 min read¬∑May 8--Ashwin RachhaText Summarization with a gRPC based Python Server and Go based Client.So you have tried and tested your Deep Learning model and it seems to work absolutely fine on your Jupyter notebook environment. That is‚Ä¶7 min read¬∑Sep 2, 2022--1See all from Ashwin RachhaRecommended from MediumPratyush KhareinMLearning.aiHow to perform High-Performance Search using FAISSA Beginner‚Äôs Guide to FAISS, use-cases, Mathematical foundations & implementation6 min read¬∑Mar 4--1Dominik PolzerinTowards Data ScienceAll You Need to Know to Build Your First LLM AppA', metadata={'source': 'https://medium.com/@ashwin_rachha/querying-a-code-database-to-find-similar-coding-problems-using-langchain-814730da6e6d', 'title': 'Querying a Code Database to find Similar Coding Problems using Langchain. | by Ashwin Rachha | Medium', 'description': 'The world of Large Language Models has taken by storm. There has been a surge in the interest and investment of LLMs by large corporations and open source developers alike due to the advancements in‚Ä¶', 'language': 'en'}), Document(page_content='using FAISSA Beginner‚Äôs Guide to FAISS, use-cases, Mathematical foundations & implementation6 min read¬∑Mar 4--1Dominik PolzerinTowards Data ScienceAll You Need to Know to Build Your First LLM AppA step-by-step tutorial to document loaders, embeddings, vector stores and prompt templates¬∑26 min read¬∑Jun 22--42ListsCoding & Development11 stories¬∑129 savesPredictive Modeling w/ Python20 stories¬∑312 savesPractical Guides to Machine Learning10 stories¬∑343 savesChatGPT21 stories¬∑124 savesYvanninBetter ProgrammingBuild a Chatbot on Your CSV Data With LangChain and OpenAIChat with your CSV file with a memory chatbot\\uf8ffü§ñ‚Ää‚Äî‚ÄäMade with Langchain\\uf8ffü¶ú and¬†OpenAI\\uf8ffüß†5 min read¬∑Jun 2--21Pankaj PandeyFaiss: Efficient Similarity Search and Clustering of Dense VectorsFaiss is a powerful library designed for efficient similarity search and clustering of dense vectors. It offers various algorithms for‚Ä¶3 min read¬∑Jun 14--Milana ShkhanukovaCosine distance and cosine similarity-Okay, Milana, there is a mistake: cosine similarity cannot be negative.', metadata={'source': 'https://medium.com/@ashwin_rachha/querying-a-code-database-to-find-similar-coding-problems-using-langchain-814730da6e6d', 'title': 'Querying a Code Database to find Similar Coding Problems using Langchain. | by Ashwin Rachha | Medium', 'description': 'The world of Large Language Models has taken by storm. There has been a surge in the interest and investment of LLMs by large corporations and open source developers alike due to the advancements in‚Ä¶', 'language': 'en'}), Document(page_content='- Oh, it can be.3 min read¬∑Mar 4--Ebo JacksoninLevel Up CodingCreate your own Chatbox and Train it in less than 30 lines of code in python using openai and‚Ä¶In this tutorial, we will explore how to create our own chatbot using Langchain and OpenAI. Langchain is a Python library that provides an‚Ä¶5 min read¬∑May 9--2See more recommendationsHelpStatusWritersBlogCareersPrivacyTermsAboutText to speechTeams', metadata={'source': 'https://medium.com/@ashwin_rachha/querying-a-code-database-to-find-similar-coding-problems-using-langchain-814730da6e6d', 'title': 'Querying a Code Database to find Similar Coding Problems using Langchain. | by Ashwin Rachha | Medium', 'description': 'The world of Large Language Models has taken by storm. There has been a surge in the interest and investment of LLMs by large corporations and open source developers alike due to the advancements in‚Ä¶', 'language': 'en'}), Document(page_content='Deploying a Slack bot with HuggingFace Transformers \\uf8ffü§ó | by Ashwin Rachha | MediumDeploying a Slack bot with HuggingFace Transformers \\uf8ffü§óAshwin Rachha¬∑Follow5 min read¬∑May 8--ListenShareSlack is a widely used team collaboration platform that enables users to communicate with their teammates, share files, and integrate third-party applications to increase productivity. A Slack bot is an automated application that can interact with users and perform certain tasks on their behalf. Slack bots can be used for a variety of purposes such as providing technical support, monitoring systems, and scheduling appointments. In this blog, we will learn how to deploy a Slack bot that can generate explanations for a given piece of code using Hugging Face Transformers, an open-source library for natural language processing.Prerequisites:A Slack workspace where the bot will be deployed.A Slack app with Socket Mode enabled.A Slack bot with the necessary permissions to send and receive messages.Hugging Face Transformers library installed.Gradio client installed.Step 1: Create a new Slack appFirst, we need to create a new Slack app in our workspace. To create a new app, go to the Slack API website and click on the ‚ÄúCreate a new app‚Äù button. Give your app a name and select the workspace where you want to deploy it.Step 2: Enable Socket ModeOnce the app is created, we need to enable Socket Mode to allow the bot to communicate with the Slack platform. To enable Socket Mode, go to the ‚ÄúApp Home‚Äù page and select ‚ÄúSocket Mode‚Äù from the left-hand menu. Toggle the switch to enable Socket Mode and save the changes.Step 3: Create a bot userNext, we need to create a bot user that will be associated with our app. To create a bot user, go to the ‚ÄúBot Users‚Äù page and click on the ‚ÄúAdd a Bot User‚Äù button. Give your bot a name and save the changes.Step 4: Install the app to your workspaceAfter creating the bot user, we need to install the app to our workspace. To install the app, go to the ‚ÄúInstall App‚Äù page and click on the ‚ÄúInstall App to Workspace‚Äù button. Review the permissions required by the app and click on the ‚ÄúAuthorize‚Äù button to grant the necessary permissions.Step 5: Get the Slack bot token and app tokenTo interact with the Slack platform, we need to obtain the Slack bot token and app token. The bot token is used to send and receive messages, while the app token is used to authenticate the app with the Slack platform. To obtain the tokens, go to the ‚ÄúInstall App‚Äù page and copy the ‚ÄúBot User OAuth Access Token‚Äù and ‚ÄúApp-Level Tokens‚Äù under the ‚ÄúOAuth & Permissions‚Äù section.Here are the steps again! Line by Line.1. Create a slack account if you do not already have one. After creating an    account, go to  [api.slack.com/apps](<http://api.slack.com/apps>)     1. Create a New App.     2. Select From Scratch and give it a name. 2. Once the app is created we have to generate some tokens i.e    The SLACK_BOT_TOKEN and the SLACK_APP_TOKEN that we will be using    in our application. 3. Before we generate the Bot token, we need to add some scopes to our Bot.   Head over to Oauth and permissions and set the following scopes:    1. chat:write    2. chat:write:public     3. Reinstall the app to the workspace.     4. Once you are done with that you get a bot token which you will        need to save somewhere. 4. Next up we want to generate an App Token.     1. Head over the Basic Information tab.     2. Go to app level    3. App-level tokens and create a new app token by giving it a name.    4. In the scope section just add connection:write  .     5. Copy and save the token. 5. Next up we need to turn on Socket mode and Enable Socket mode. 6. Last thing to do is select Interactivity & Shortcuts and make sure    it is enabled.7. Final thing to do is go to Event Subscriptions and subscribe to    whatever events you want. For example if you want your bot to listen    whenever it is mentioned then select', metadata={'source': 'https://medium.com/@ashwin_rachha/deploying-a-slack-bot-with-huggingface-transformers-6962ec0fba44', 'title': 'Deploying a Slack bot with HuggingFace Transformers \\uf8ffü§ó | by Ashwin Rachha | Medium', 'description': 'Slack is a widely used team collaboration platform that enables users to communicate with their teammates, share files, and integrate third-party applications to increase productivity. A Slack bot is‚Ä¶', 'language': 'en'}), Document(page_content='it is enabled.7. Final thing to do is go to Event Subscriptions and subscribe to    whatever events you want. For example if you want your bot to listen    whenever it is mentioned then select app_mention.Step 6: Create a Python scriptNow, let‚Äôs create a Python script that will handle the interactions between the bot and the Slack platform. We will use the Slack Bolt library, which is a Python framework for building Slack bots. We will also use the Hugging Face Transformers library to generate explanations for a given piece of code.import osfrom slack_bolt import Appfrom slack_bolt.adapter.socket_mode import SocketModeHandlerfrom transformers import pipelinefrom gradio_client import Clientimport requestsos.environ[\"SLACK_BOT_TOKEN\"] = \"YOUR SLACK BOT KEY\"os.environ[\"SLACK_APP_TOKEN\"] = \"YOUR SLACK APP KEY\"def generate_text(text):    ans = \"Server Error\"    response = requests.post(\"<https://ashwinr-pythoncodeexplainer.hf.space/run/predict>\", json={        \"data\": [            text        ]    }).json()    print(response)    if \"data\" not in response:        return ans    data = response[\\'data\\'][0]    print(data)    ans = f\"Output : {data}\"    return ansapp = App(token=os.environ[\"SLACK_BOT_TOKEN\"])@app.command(\"/explain_code\")def handle_explain_code(ack, body, logger):    ack()    text = \" \".join(body[\"text\"].split()[1:])    if not text.strip():        return \"Please provide some text to explain.\"    generated_text = generate_text(text)    response = f\"Explanation for \\'{text}\\':\\\\\\\\n{generated_text}\"    return response@app.event(\"message\")def handle_message(event, say):    text = event[\"text\"]    if not text.strip():        say(\"Please provide some text to explain.\")        return    generated_text = generate_text(text)    say(generated_text)handler = SocketModeHandler(app_token=os.environ[\"SLACK_APP_TOKEN\"], app=app)handler.start()Note:You can use any underlying language model to generate text. Here I am using one from my own HuggingFace space : https://huggingface.co/spaces/ashwinR/PythonCodeExplainerWhich uses a CodeT5 model under the hood.You can also use OpenAI‚Äôs GPT3 by providing in an API key.def generate_text(input_text):    response = openai.Completion.create(        engine=\"text-davinci-002\",        prompt=input_text,        temperature=0.5,        max_tokens=1000,        n = 1,        stop=None,        timeout=20,    )    return response.choices[0].text.strip()Deploying the Slack Bot using RenderCreate a new Render account and log in to the dashboard.Click on ‚ÄúNew Web Service‚Äù and select ‚ÄúDockerfile‚Äù as the deployment method.Next, give a name to the service and select the repository where the code of the Slack bot is present.In the ‚ÄúEnvironment‚Äù section, add the Slack bot token and app token as environment variables.After adding the environment variables, click on ‚ÄúCreate Web Service‚Äù to deploy the code.Once the deployment is complete, you can start using the Slack bot.Deploying the Slack Bot using VercelCreate a new Vercel account and log in to the dashboard.Click on ‚ÄúNew Project‚Äù and select ‚ÄúImport Git Repository‚Äù.Choose the repository where the code of the Slack bot is present and click on ‚ÄúImport‚Äù.Once the repository is imported, create a new environment variable for the Slack bot token.Next, click on ‚ÄúDeploy‚Äù to deploy the code on Vercel.After the deployment is complete, you can test the Slack bot.OUTPUT:Code Source : https://github.com/RachhaAshwin/SlackBotSlackChatbotsNLPOpenAILlm----FollowWritten by Ashwin Rachha18 FollowersFollowMore from Ashwin RachhaAshwin RachhaQuerying a Code Database to find Similar Coding Problems using Langchain.The Dawn of the Planet of Language Models8 min read¬∑Apr 24--2Ashwin RachhaPatent Phrase-to-Phrase Matching with Pytorch LightningKaggle Recently Launched an NLP based competition wherein competitors are required to extract meaning from a large text-based dataset for‚Ä¶9 min read¬∑May 11, 2022--Ashwin RachhaAddition and Subtraction using', metadata={'source': 'https://medium.com/@ashwin_rachha/deploying-a-slack-bot-with-huggingface-transformers-6962ec0fba44', 'title': 'Deploying a Slack bot with HuggingFace Transformers \\uf8ffü§ó | by Ashwin Rachha | Medium', 'description': 'Slack is a widely used team collaboration platform that enables users to communicate with their teammates, share files, and integrate third-party applications to increase productivity. A Slack bot is‚Ä¶', 'language': 'en'}), Document(page_content='Launched an NLP based competition wherein competitors are required to extract meaning from a large text-based dataset for‚Ä¶9 min read¬∑May 11, 2022--Ashwin RachhaAddition and Subtraction using Recurrent Neural Networks.How does google understand how to translate ‚Äò‰ªäÊó•„ÅØ„Å©„ÅÜ„Åß„Åô„ÅãÔºü‚Äô to ‚ÄòHow are you doing today?‚Äô or vice versa? How do we get to predict a disease‚Ä¶11 min read¬∑Jan 23, 2022--Ashwin RachhaText Summarization with a gRPC based Python Server and Go based Client.So you have tried and tested your Deep Learning model and it seems to work absolutely fine on your Jupyter notebook environment. That is‚Ä¶7 min read¬∑Sep 2, 2022--1See all from Ashwin RachhaRecommended from MediumAlissainPython in Plain EnglishRun Your Own LLM-Powered Chatbot in 5 Easy Steps\\uf8ffü§ñ = Streamlit + Langchain \\uf8ffü¶úÔ∏è + LLaMA \\uf8ffü¶ô3 min read¬∑Jul 21--Mohit SoniHow to use Chat GPT and LangChain to get responses from your documentsIn the previous post, I explained methods that you can use to train Chat GPT on your data. In this post, we will use one of those methods‚Ä¶4 min read¬∑Jun 17--ListsNatural Language Processing551 stories¬∑175 savesNow in AI: Handpicked by Better Programming266 stories¬∑108 savesThe New Chatbots: ChatGPT, Bard, and Beyond13 stories¬∑96 savesAI Regulation6 stories¬∑90 savesJason WebsterChatting to a Website with OpenAI, LangChain, and ChromaDBLarge language models (LLMs) are proving to be a powerful generational tool and assistant that can handle a large variety of questions and‚Ä¶13 min read¬∑Aug 4--Sidharth RampallyDeploying a Productivity Q&A Bot with Retrieval Augmented Generation (RAG) on SageMaker JumpstartDeveloped for my peers using LangChain and Streamlit7 min read¬∑Jul 17--Ebo JacksoninLevel Up CodingCreate your own Chatbox and Train it in less than 30 lines of code in python using openai and‚Ä¶In this tutorial, we will explore how to create our own chatbot using Langchain and OpenAI. Langchain is a Python library that provides an‚Ä¶5 min read¬∑May 9--2Abhishek RaiBuild AI Q&A Chatbot for your own docs.Wouldn‚Äôt it be nice if we can simply upload some PDFs, Docs, and URLs to someplace and then have a chatbot which can answer any questions‚Ä¶4 min read¬∑Jun 17--1See more recommendationsHelpStatusWritersBlogCareersPrivacyTermsAboutText to speechTeams', metadata={'source': 'https://medium.com/@ashwin_rachha/deploying-a-slack-bot-with-huggingface-transformers-6962ec0fba44', 'title': 'Deploying a Slack bot with HuggingFace Transformers \\uf8ffü§ó | by Ashwin Rachha | Medium', 'description': 'Slack is a widely used team collaboration platform that enables users to communicate with their teammates, share files, and integrate third-party applications to increase productivity. A Slack bot is‚Ä¶', 'language': 'en'}), Document(page_content='Text Summarization with a gRPC based Python Server and Go based Client. | by Ashwin Rachha | MediumText Summarization with a gRPC based Python Server and Go based Client.Ashwin Rachha¬∑Follow7 min read¬∑Sep 2, 2022--1ListenSharePhoto by Aaron Burden on UnsplashSo you have tried and tested your Deep Learning model and it seems to work absolutely fine on your Jupyter notebook environment. That is amazing! What now? Would you want to keep the model weights unused after demonstrating your ‚ÄúProof of Concept‚Äù project only to put it into the recycle bin? Or Would you like to deploy it online so that it is persisted and can be used by anyone with access to it? Of course I am guessing you would choose the latter option.In this blog we shall go over how can we take the first steps towards productionizing your NLP models. We will be creating a gRPC server which will have a Text Summarization model running in the background and we will be using a Go based client to communicate with the server via gRPC requests and responses.We can have the best of Python and Golang using gRPC (google Remote Procedure Call). This will allow us to build a simple client in golang and parallely we shall create a gRPC based python server which will invoke a huggingface pipeline for Text Summarization. The go client will send input long text to the server and the server shall fetch the abridged version of the text.A brief Overview of gRPCIn gRPC a client application can directly call a method on a server application on a different machine as if it were a local object making it easier to create distributed applications and services. As with other RPC based systems, gRPC is based around specifying a service implementation, specifying the methods that can be called remotely with the apt function signature. On the server side it implements the server interface and runs a gRPC server to handle client calls. gRPC uses protocol buffers for serializing structured data to send over a network. It can also be used with other formats such as JSON. The format is suitable for both ephemeral network traffic and long-term data storage.Defining the ProtofilesWe will first start our project by defining a summarization.proto file that will help use define the protocol buffers as well as the service interface.syntax = \"proto3\";package pb;option go_package = \"./pb\";message SummaryRequest {  string request = 1;}message SummaryResponse {  repeated string response = 1;}service Summarization {  rpc GetSummary(SummaryRequest) returns (SummaryResponse);}The first line specifies that we are using the proto3 syntax. If we dont do this the protocol buffer compiler will assume the version to be proto2.We also set the package pb to our directory and set the go_package as relative path ‚Äú./pb‚ÄùFinally we define the Proto Request and Proto Response messages in our file. This is nothing but the specification for the structure of data that will be serialized and sent over the network.The Summary request will be one long string containing the document long text.The summary response will be an array of strings that will store the tokens of the response summary. Since the output is an array we use the term repeated before the string definition. We can also declare it as a single string variable.Generating the protobuf and gRPC code$ protoc --go_out=. --go_opt=paths=source_relative \\\\\\\\    --go-grpc_out=. --go-grpc_opt=paths=source_relative \\\\\\\\    project/summarization.protoIn order to compile the .proto files and generate gRPC code we need to run the above mentioned command. The protoc command is the compiler for protocol buffers ‚Äî the format of data that gRPC uses. The flag go_out=plugins=grpc:pb tells protoc to use the gRPC plugin and place the files in the pb directory. The go_opt=paths=source_relative tells protoc to generate the code in the pb directory relative to the current directory. The summarization.proto file will be used to generate the code. This will fill out summarization.pb.go', metadata={'source': 'https://medium.com/@ashwin_rachha/text-summarization-with-a-grpc-based-python-server-and-go-based-client-3bbe9badd936', 'title': 'Text Summarization with a gRPC based Python Server and Go based Client. | by Ashwin Rachha | Medium', 'description': 'So you have tried and tested your Deep Learning model and it seems to work absolutely fine on your Jupyter notebook environment. That is amazing! What now? Would you want to keep the model weights‚Ä¶', 'language': 'en'}), Document(page_content='tells protoc to generate the code in the pb directory relative to the current directory. The summarization.proto file will be used to generate the code. This will fill out summarization.pb.go with data bindings and helper methods.$ python3 -m grpc_tools.protoc -I.. --python_out=. --grpc_python_out=. ../summarization.protoWe can finally generate Python code. This creates code for summarization_pb2_grpc.py and summarization_pb2.py. We can move them to the file where the server logic will be written i.e server.py file.Server Logicfrom concurrent.futures import ThreadPoolExecutorfrom summarization_pb2 import SummaryResponsefrom summarization_pb2_grpc import SummarizationServicer, add_SummarizationServicer_to_serverimport loggingimport grpc import torchimport warningswarnings.filterwarnings(action = \\'ignore\\')from transformers import AutoTokenizer, AutoModelWithLMHeadclass ModelInit:    def __init__(self, model_name = \\'t5-base\\'):        self.tokenizer = AutoTokenizer.from_pretrained(\\'t5-base\\')        self.model = AutoModelWithLMHead.from_pretrained(\\'t5-base\\')class SummarizationService(SummarizationServicer):    def __init__(self):        pass    def GetSummary(self, request):        logging.info(f\"Full Text\" + self.request)        text = self.request        t5Model = ModelInit()        tokenizer = t5Model.tokenizer        model = t5Model.model        inputs = tokenizer.encode(            \"summarize: \" + text,            return_tensors = \\'pt\\',            max_length = 512,            truncation = True         )        summary_ids = model.generate(            inputs,             max_length = 150,            min_length = 80,            length_penalty=5., num_beams=2        )        summary = tokenizer.batch_decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)        resp = SummaryResponse(response = summary)        return respif __name__ == \"__main__\":    logging.basicConfig(        level = logging.INFO,        format=\\'%(asctime)s - %(levelname)s - %(message)s\\',    )    server = grpc.server(ThreadPoolExecutor())    add_SummarizationServicer_to_server(SummarizationService, server)    port = 8080    server.add_insecure_port(f\\'[::]:{port}\\')    server.start()    logging.info(f\\'Server Ready on Port {port}\\')    server.wait_for_termination()Lets go over the code line by line:We define two seperate classes ‚Äî one which will initialize the model and tokenizer for us and another class which is the SummarizationService class which inherits the SummarizationServicer class which was created in summarization_pb2_grpc. This is all part of creating implementations for our Remote Procedure calls.To write the Python service, you need to inherit from the SummarizationService defined in summarization_pb2_grpc.py and override the GetSummary method with our custom implementation.The GetSummary is a simple function which will declare the model as well as tokenizer for our Summarization Service. In this example we are using the T5ForConditionalGeneration model and T5Tokenizer for outputing generated tokens and tokenizing words respectively.The pipeline is pretty simple. Encode the given text ‚Üí send it to the model ‚Üí Convert the generated ids back to decoded words. After which we simply embed this in the SummaryResponse message imported from the file generated.We instantiate the server in the main function. We define the grpc server with a Threadpool Executor. gRPC Python server currently will package each RPC request as a future, and submit it to the thread pool. If the thread pool runs out of available thread, it will spawn a new one and process the future. In another word, each RPC will run in a dedicated thread, and if the RPC is blocked the thread will be blocked as well.The add summarization service will add the service to the instantiated server and the server will start on a port that we have specified until an external interrupt is found.And that is it! We are ready to write our client code to get some', metadata={'source': 'https://medium.com/@ashwin_rachha/text-summarization-with-a-grpc-based-python-server-and-go-based-client-3bbe9badd936', 'title': 'Text Summarization with a gRPC based Python Server and Go based Client. | by Ashwin Rachha | Medium', 'description': 'So you have tried and tested your Deep Learning model and it seems to work absolutely fine on your Jupyter notebook environment. That is amazing! What now? Would you want to keep the model weights‚Ä¶', 'language': 'en'}), Document(page_content='service to the instantiated server and the server will start on a port that we have specified until an external interrupt is found.And that is it! We are ready to write our client code to get some summaries!Writing the client code.package mainimport (\\t\"context\"\\t\"log\"\\t\"main/pb\"\\t\"google.golang.org/grpc\")func main() {\\taddr := \"localhost:8080\"\\tconn, err := grpc.Dial(addr, grpc.WithInsecure(), grpc.WithBlock())\\tif err != nil {\\t\\tlog.Fatal(err)\\t}\\tdefer conn.Close()\\tclient := pb.NewSummarizationClient(conn)\\tinp_str := \"A theme is any universal idea explored in a literary work. After reading the novel Lolita it became obvious that there were multiple themes occurring throughout the book.In my eyes the most important theme of them all was the power of diction and how Nabokov honored words because they elevated his artwork otherwise dreadful topic. This particular book is known for being risqu√©, but it is important to note that there are no four-letter words or any obvious graphic material; that\\'s because of Humbert\\'s word choice. The language used in Lolita successfully overcompensates the unadvisable content and allows a sense of beauty to prevail. Subjects such as murder, pedophilia, rape, and even incest are surprisingly appealing due to the way Humbert Humbert narrates each scene with powerful word choice. Humbert uses diction and other forms of diction such as alliteration and imagery to ensure the captivation of his readers, entangling and convincing them into buying his version of the confession.\"\\treq := pb.SummaryRequest{\\t\\tRequest: inp_str,\\t}\\tresp, err := client.GetSummary(context.Background(), &req)\\tif err != nil {\\t\\tlog.Fatal(err)\\t}\\tlog.Printf(\"Predicted Summary: %v\", resp.Response)}Writing the golang client code is pretty easy.We first import the go file generated by the protoc compiler which has the SummaryRequest and SummaryResponse defined in it.We will dial a connection the port on which the server is on and define the client object. The client interface is defined in summarization.pb.go file astype SummarizationClient interface {\\tGetSummary(ctx context.Context, in *SummaryRequest, opts ...grpc.CallOption) (*SummaryResponse, error)}3. Next we simply define our document string, invoke the GetSummary function and print out the response.Output:To query our gRPC server we can use BloomRPC.The complete code can be found here : https://github.com/AshwinRachha/golangSummarizationThank you for reading!NaturallanguageprocessingMlopsGrpcMachine LearningClient Server----1FollowWritten by Ashwin Rachha18 FollowersFollowMore from Ashwin RachhaAshwin RachhaQuerying a Code Database to find Similar Coding Problems using Langchain.The Dawn of the Planet of Language Models8 min read¬∑Apr 24--2Ashwin RachhaPatent Phrase-to-Phrase Matching with Pytorch LightningKaggle Recently Launched an NLP based competition wherein competitors are required to extract meaning from a large text-based dataset for‚Ä¶9 min read¬∑May 11, 2022--Ashwin RachhaAddition and Subtraction using Recurrent Neural Networks.How does google understand how to translate ‚Äò‰ªäÊó•„ÅØ„Å©„ÅÜ„Åß„Åô„ÅãÔºü‚Äô to ‚ÄòHow are you doing today?‚Äô or vice versa? How do we get to predict a disease‚Ä¶11 min read¬∑Jan 23, 2022--Ashwin RachhaDeploying a Slack bot with HuggingFace Transformers \\uf8ffü§óSlack is a widely used team collaboration platform that enables users to communicate with their teammates, share files, and integrate‚Ä¶5 min read¬∑May 8--See all from Ashwin RachhaRecommended from MediumZeeshan MalikConnecting ChatGPT with your own Data using Llama Index and LangChainIn the last three months, there has been a rapid increase in the use of Large Language Models (LLMs) for a variety of applications, such as‚Ä¶5 min read¬∑Jun 11--2Dmitry KruglovinBetter ProgrammingThe Architecture of a Modern StartupHype wave, pragmatic evidence vs the need to move fast16 min read¬∑Nov 7, 2022--47ListsPredictive Modeling w/ Python20 stories¬∑316 savesPractical Guides to Machine Learning10', metadata={'source': 'https://medium.com/@ashwin_rachha/text-summarization-with-a-grpc-based-python-server-and-go-based-client-3bbe9badd936', 'title': 'Text Summarization with a gRPC based Python Server and Go based Client. | by Ashwin Rachha | Medium', 'description': 'So you have tried and tested your Deep Learning model and it seems to work absolutely fine on your Jupyter notebook environment. That is amazing! What now? Would you want to keep the model weights‚Ä¶', 'language': 'en'}), Document(page_content='of a Modern StartupHype wave, pragmatic evidence vs the need to move fast16 min read¬∑Nov 7, 2022--47ListsPredictive Modeling w/ Python20 stories¬∑316 savesPractical Guides to Machine Learning10 stories¬∑347 savesNatural Language Processing551 stories¬∑175 savesThe New Chatbots: ChatGPT, Bard, and Beyond13 stories¬∑96 savesManoj Kumar DhakadPython: Read and send Outlook mail using OAuth2 token and Graph APIMicrosoft has announced the deprecation of basic authentication for Outlook, which involves using a username and password for‚Ä¶7 min read¬∑Jun 30--2Dominik PolzerinTowards Data ScienceAll You Need to Know to Build Your First LLM AppA step-by-step tutorial to document loaders, embeddings, vector stores and prompt templates¬∑26 min read¬∑Jun 22--42Octavian ZarzuBuild and deploy apps with DuckDB and Streamlit in under one hourFor me‚Ää‚Äî‚Ääsomeone primarily comfortable in writing SQL‚Ää‚Äî‚ÄäI longed for a setup that would allow me to easily build data apps off some data‚Ä¶11 min read¬∑Feb 27--Jason WebsterChatting to a Website with OpenAI, LangChain, and ChromaDBLarge language models (LLMs) are proving to be a powerful generational tool and assistant that can handle a large variety of questions and‚Ä¶13 min read¬∑Aug 4--See more recommendationsHelpStatusWritersBlogCareersPrivacyTermsAboutText to speechTeams', metadata={'source': 'https://medium.com/@ashwin_rachha/text-summarization-with-a-grpc-based-python-server-and-go-based-client-3bbe9badd936', 'title': 'Text Summarization with a gRPC based Python Server and Go based Client. | by Ashwin Rachha | Medium', 'description': 'So you have tried and tested your Deep Learning model and it seems to work absolutely fine on your Jupyter notebook environment. That is amazing! What now? Would you want to keep the model weights‚Ä¶', 'language': 'en'}), Document(page_content=\"About me:\\n\\nI am a Master's student majoring in Computer Science at Virginia Tech. I have a passion for building, scaling, maintaining and perusing about intelligent systems that have sprung from the fields of Machine Learning, Natural Language Processing and principles of Software Engineering with their presence in real-world settings. \\n\\nMy most recent experience was a wonderful internship experience at Outreach Corporation where I was responsible for building a template engine project that could ease the process of Deploying NLP based models online for Data Scientists and Machine Learning Engineers. During this experience I learned how to build and configure a complex Inference Service Solution (NVIDIA Triton Inference Server) for deploying Model Binaries for BERT, ROBERTA and DistilBERT in their ONNX flavor. I also wrote a Golang Based Microservice which would be used to communicate with the configured server and would server prediction results in real-time. Finally to test the working of the project I wrote tests in CircleCI configs and dockerized the inference solution as well as the microservice into a container and deployed it online via Kubernetes manifests. Being in a close knit environment with great leaders and programmers was the best highlight of my internship. This experience evolved my thinking about designing and deploying systems at scale . It also goes without saying that I had an opportunity to add a lot of technologies to my belt ranging from Golang, CircleCI, Kubernetes, Docker, Bash scripting, Python scripting and Software Engineering tools such as Git, Github, JIRA, Confluence etc. \\n\\nI love writing about and elucidating Machine Learning and Deep learning concepts and code, it sort of acts as a mental note to myself and as a result I blog regularly @ https://ashwinrachha.github.io/blogpost/ and https://medium.com/@ashwin_rachha/ . I am a Kaggle Notebooks experts and enjoy writing code related to Data Science, NLP and CV competitions. Please checkout my kaggle profile @ https://www.kaggle.com/ashwinrachha1. Alternatively I love solving Data Structures and Algorithms related problems and frequenty blog about them. Checkout my Leetcode profile @ https://leetcode.com/ashwin_rachha/\\n\\nI am open for Full time opportunities in Machine Learning and or Software Development starting in the summer of 2023. Please feel free to reach out to me regarding any opportunity or for collaborations or just for the sake of reaching out :D.\\n\\n\\nExperience:\\n\\n\\nExperienceExperience\\n\\nVirginia Tech Department of Computer Science logo\\nVirginia Tech Department of Computer ScienceVirginia Tech Department of Computer Science\\nFull-time · 10 mosFull-time · 10 mos\\nGraduate Teaching Assistant - Software EngineeringGraduate Teaching Assistant - Software Engineering\\nJan 2023 - May 2023 · 5 mosJan 2023 - May 2023 · 5 mos\\nBlacksburg, Virginia, United States · On-siteBlacksburg, Virginia, United States · On-site\\nGraduate Teaching Assistant for the Course CS5704 - Software Engineering.\\n\\nResponsibilities:\\n\\nAssisted students with assignments related to Java and Kotlin, providing guidance and support to ensure successful completion.\\nExplained complex software engineering concepts, including design patterns, through clear and concise explanations, fostering student understanding and application.\\nConducted grading of assignments, ensuring fairness and accuracy in evaluating student work based on established rubrics and guidelines.\\nActively participated in brainstorming sessions, collaborating with course instructors to develop engaging discussions and assignments that enhanced student learning experience.\\nOffered one-on-one consultations to address students' questions and concerns, providing personalized assistance to aid in their academic success.\\nMaintained effective communication with students, promptly responding to inquiries and providing timely feedback to facilitate their learning process.\\n✨ Achievements:\", metadata={'source': '/content/ashwin.txt'}), Document(page_content=\"Received positive feedback from students for exceptional teaching skills, resulting in improved course evaluations and overall student satisfaction.\\nCollaborated with faculty members to create and implement innovative teaching strategies, incorporating real-world examples and hands-on activities to enhance student engagement and comprehension.\\nDeveloped additional resources, such as coding tutorials and study guides, to supplement course materials and further support student learning.\\n\\n💡 Technical Skills:\\n\\nProgramming Languages: Java, Kotlin.\\nTools: Github, Gitlab.Graduate Teaching Assistant for the Course CS5704 - Software Engineering. Responsibilities: Assisted students with assignments related to Java and Kotlin, providing guidance and support to ensure successful completion. Explained complex software engineering concepts, including design patterns, through clear and concise explanations, fostering student understanding and application. Conducted grading of assignments, ensuring fairness and accuracy in evaluating student work based on established rubrics and guidelines. Actively participated in brainstorming sessions, collaborating with course instructors to develop engaging discussions and assignments that enhanced student learning experience. Offered one-on-one consultations to address students' questions and concerns, providing personalized assistance to aid in their academic success. Maintained effective communication with students, promptly responding to inquiries and providing timely feedback to facilitate their learning process. ✨ Achievements: Received positive feedback from students for exceptional teaching skills, resulting in improved course evaluations and overall student satisfaction. Collaborated with faculty members to create and implement innovative teaching strategies, incorporating real-world examples and hands-on activities to enhance student engagement and comprehension. Developed additional resources, such as coding tutorials and study guides, to supplement course materials and further support student learning. 💡 Technical Skills: Programming Languages: Java, Kotlin. Tools: Github, Gitlab.\\nSkills: Kotlin · Algorithms · Continuous Integration and Continuous Delivery (CI/CD) · Software Development · JavaSkills: Kotlin · Algorithms · Continuous Integration and Continuous Delivery (CI/CD) · Software Development · Java\\nGraduate Teaching Assistant - Introduction to Data Analytics and VisualizationGraduate Teaching Assistant - Introduction to Data Analytics and Visualization\\nAug 2022 - Jan 2023 · 6 mosAug 2022 - Jan 2023 · 6 mos\\nResponsibilities:\\n\\nAssisted students in understanding and applying data science and machine learning concepts, including regression, classification, clustering, statistics, and natural language processing.\\nDesigned assignments that challenged students to apply their knowledge and skills in real-world scenarios, fostering critical thinking and problem-solving abilities.\\nGraded assignments and provided constructive feedback to help students improve their understanding and implementation of data analytics and visualization techniques.\\nConducted weekly office hours, offering personalized guidance and clarification on complex topics related to machine learning and data science.\\nCollaborated with course instructors to brainstorm and develop engaging discussions and activities that enhanced students' comprehension and practical skills in data analytics and visualization.\\n✨ Achievements:\", metadata={'source': '/content/ashwin.txt'}), Document(page_content=\"Recognized for exceptional teaching skills and ability to simplify complex concepts, resulting in positive feedback from students and improved course evaluations.\\nContributed to the development of a comprehensive curriculum that integrated cutting-edge techniques and tools in data analytics and visualization, preparing students for real-world applications.\\nLed workshops on machine learning algorithms and data analysis techniques, providing students with hands-on experience in utilizing popular libraries and software tools.\\n\\n\\n💡 Technical Skills:\\n\\nProgramming Languages: Python, R\\nData Analytics Tools: Pandas, NumPy, Scikit-learn, TensorFlow, Keras\\nData Visualization: Tableau, Matplotlib, Seaborn\\nMachine Learning Concepts: Regression, Classification, Clustering, Dimensionality Reduction, Natural Language Processing, Deep Learning\\nStatistical Analysis: Hypothesis Testing, Descriptive Statistics, Probability Distributions Teaching Assistant for the course CS3654 - Introduction to Data Analytics and Visualization.Responsibilities: Assisted students in understanding and applying data science and machine learning concepts, including regression, classification, clustering, statistics, and natural language processing. Designed assignments that challenged students to apply their knowledge and skills in real-world scenarios, fostering critical thinking and problem-solving abilities. Graded assignments and provided constructive feedback to help students improve their understanding and implementation of data analytics and visualization techniques. Conducted weekly office hours, offering personalized guidance and clarification on complex topics related to machine learning and data science. Collaborated with course instructors to brainstorm and develop engaging discussions and activities that enhanced students' comprehension and practical skills in data analytics and visualization. ✨ Achievements: Recognized for exceptional teaching skills and ability to simplify complex concepts, resulting in positive feedback from students and improved course evaluations. Contributed to the development of a comprehensive curriculum that integrated cutting-edge techniques and tools in data analytics and visualization, preparing students for real-world applications. Led workshops on machine learning algorithms and data analysis techniques, providing students with hands-on experience in utilizing popular libraries and software tools. 💡 Technical Skills: Programming Languages: Python, R Data Analytics Tools: Pandas, NumPy, Scikit-learn, TensorFlow, Keras Data Visualization: Tableau, Matplotlib, Seaborn Machine Learning Concepts: Regression, Classification, Clustering, Dimensionality Reduction, Natural Language Processing, Deep Learning Statistical Analysis: Hypothesis Testing, Descriptive Statistics, Probability Distributions Teaching Assistant for the course CS3654 - Introduction to Data Analytics and Visualization.\\nSkills: Python · Deep Learning · Artificial Intelligence (AI) · Pandas (Software) · Natural Language Processing (NLP)\\n\\n\\n\\n\\nMachine Learning InternMachine Learning Intern\\nOutreach · Full-timeOutreach · Full-time\\nJun 2022 - Aug 2022 · 3 mosJun 2022 - Aug 2022 · 3 mos\\nSeattle, Washington, United StatesSeattle, Washington, United States\\n● Responsible for developing a template engine project to help Data Scientists and Machine Learning Engineers at Outreach to use templates to deploy any NLP model online inorder for them to avoid writing redundant boiler plate code.\\n\\n● Delivered an Online Inference Solution with a gRPC based Microservice in Golang serving NLP based models viz. BERT, ROBERTA and DISTILBERT for topic detection, question detection, action analysis and sentiment analysis.\\n\\n● Wrote Python pipelines for ingesting data, preprocessing, tokenization, prediction and postprocessing of text data.\", metadata={'source': '/content/ashwin.txt'}), Document(page_content='● Wrote Python pipelines for ingesting data, preprocessing, tokenization, prediction and postprocessing of text data.\\n\\n● Wrote Bash scripts to instantiate NLP model binaries in the ONNX format on the NVIDIA Triton Inference Server and packaged the inference solution as a docker image.\\n\\n● Wrote a Go based microservice to be used to communicate with the inference server via gRPC requests and responses. Dockerized the microservice solution which would be later used to communicate with the inference service.\\n\\n● Wrote tests for the application service as well as the inference service via CircleCI configuration files.\\n\\n● Deployed the application online via Kubernetes manifests on Outreach Staging Environment.\\n\\n● Reduced Data Scientist efficiency time from 3-4 days to 2 Hours.● Responsible for developing a template engine project to help Data Scientists and Machine Learning Engineers at Outreach to use templates to deploy any NLP model online inorder for them to avoid writing redundant boiler plate code. ● Delivered an Online Inference Solution with a gRPC based Microservice in Golang serving NLP based models viz. BERT, ROBERTA and DISTILBERT for topic detection, question detection, action analysis and sentiment analysis. ● Wrote Python pipelines for ingesting data, preprocessing, tokenization, prediction and postprocessing of text data. ● Wrote Bash scripts to instantiate NLP model binaries in the ONNX format on the NVIDIA Triton Inference Server and packaged the inference solution as a docker image. ● Wrote a Go based microservice to be used to communicate with the inference server via gRPC requests and responses. Dockerized the microservice solution which would be later used to communicate with the inference service. ● Wrote tests for the application service as well as the inference service via CircleCI configuration files. ● Deployed the application online via Kubernetes manifests on Outreach Staging Environment. ● Reduced Data Scientist efficiency time from 3-4 days to 2 Hours.\\nSkills: Data Engineering · Google Cloud Platform (GCP) · Algorithms · Git · Backend Engineering · Microservices · Online Inference · Docker · Kubernetes · CircleCI · Continuous Integration and Continuous Delivery (CI/CD) · Software Development · MLOps · Go (Programming Language) · Python (Programming Language) · Machine Learning · Deep Learning · Natural Language Processing (NLP)\\n\\n\\n\\n\\nSoftware EngineerSoftware Engineer\\nMindbowser Inc · Full-timeMindbowser Inc · Full-time\\nDec 2019 - Feb 2021 · 1 yr 3 mosDec 2019 - Feb 2021 · 1 yr 3 mos\\nPune, Maharashtra, IndiaPune, Maharashtra, India\\n● Responsible for implementing a Facial Expression Recognition (FER) application.\\n\\n● Performed Exploratory Data Analysis on the underlying data - (FER 2013 dataset) and tested classical Machine Learning models viz. Logistic Regression, Support Vector Machine as a baseline classifier.\\n\\n● Implemented a Proof of Concept Convolutional Neural Network VGG-19 transfer learning model as the final classifier in Pytorch achieving an accuracy of 73% on the validation set.\\n\\n● Integrated the application with a MongoDB database to store meeting metadata (timestamps, faces detected, expressions classified etc) and respective images in a GridFS format.\\n\\n● Wrote a GUI script to translate the POC into a desktop application using Python Tkinter.\\n\\n● Alternatively wrote a Flask application to build a web application on the underlying model and dockerized the application.\\n\\n● Packaged the code in a python based executable which could be instantiated with a button click on the desktop as an application.\\n\\n● Owned the application from design, development to production.', metadata={'source': '/content/ashwin.txt'}), Document(page_content=\"● Packaged the code in a python based executable which could be instantiated with a button click on the desktop as an application.\\n\\n● Owned the application from design, development to production.\\n\\n● The project is in beta testing at Volkswagen and Bajaj India.● Responsible for implementing a Facial Expression Recognition (FER) application. ● Performed Exploratory Data Analysis on the underlying data - (FER 2013 dataset) and tested classical Machine Learning models viz. Logistic Regression, Support Vector Machine as a baseline classifier. ● Implemented a Proof of Concept Convolutional Neural Network VGG-19 transfer learning model as the final classifier in Pytorch achieving an accuracy of 73% on the validation set. ● Integrated the application with a MongoDB database to store meeting metadata (timestamps, faces detected, expressions classified etc) and respective images in a GridFS format. ● Wrote a GUI script to translate the POC into a desktop application using Python Tkinter. ● Alternatively wrote a Flask application to build a web application on the underlying model and dockerized the application. ● Packaged the code in a python based executable which could be instantiated with a button click on the desktop as an application. ● Owned the application from design, development to production. ● The project is in beta testing at Volkswagen and Bajaj India.\\nSkills: Flask · Graphical User Interface (GUI) · Tkinter · OpenCV · MongoDB · Docker · Python (Programming Language) · Deep Learning · PyTorch · Keras · Scikit-Learn\\n\\n\\n\\nEducationEducation\\nVirginia Tech logo\\nVirginia TechVirginia Tech\\nMaster's degree, Computer ScienceMaster's degree, Computer Science\\nAug 2021 - May 2023Aug 2021 - May 2023\\nGrade: 4.0/4.0Grade: 4.0/4.0\\nSkills: KotlinSkills: Kotlin\\nPune Institute of Computer Technology logo\\nPune Institute of Computer TechnologyPune Institute of Computer Technology\\nBachelor's degree, Computer EngineeringBachelor's degree, Computer Engineering\\n2016 - 20202016 - 2020\\nGrade: 8.7 GPAGrade: 8.7 GPA\\nFergusson College logo\\nFergusson CollegeFergusson College\\nHSCHSC\\nJun 2014 - Feb 2016Jun 2014 - Feb 2016\\nGrade: 83%\\n\\n\\nMy Recommendations:\\n\\nRecommendationsRecommendations\\n\\nReceivedReceived\\nGivenGiven\\nSandeep Natoo profile picture\\nSandeep NatooSandeep Natoo\\n· 1stFirst degree connection\\nMachine Learning Engineer at MindbowserMachine Learning Engineer at Mindbowser\\nI am fortunate that I came across talent like Ashwin. I had a pleasure working with Ashwin on his internship period. We worked together on one of our emerging machine learning project, i. e. detecting & identifying human facial expressions.\\n\\nWe really thankful for his contribution in developing the deep leaning model. Apart from his data science skills, he was very friendly & helpful with the team.\\n\\nI wish him good luck for his future.\\nThank you!I am fortunate that I came across talent like Ashwin. I had a pleasure working with Ashwin on his internship period. We worked together on one of our emerging machine learning project, i. e. detecting & identifying human facial expressions. We really thankful for his contribution in developing the deep leaning model. Apart from his data science skills, he was very friendly & helpful with the team. I wish him good luck for his future. Thank you!…see more\\nShubham Yadav profile picture\\nShubham YadavShubham Yadav\\n· 1stFirst degree connection\\nDevelopment Team Lead at Mindbowser IncDevelopment Team Lead at Mindbowser Inc\\nSeptember 23, 2021, Shubham was Ashwin’s mentorSeptember 23, 2021, Shubham was Ashwin’s mentor\\nI had the pleasure of working with Ashwin when he interned at Mindbowser Info solutions Pvt. Ltd. He demonstrated great skill in Python, OpenCV and Machine Learning concepts. I am confident in his ability of making big in the industry. He is focused and commited to his work. I wish him well in his endeavors.\\n\\nMy Cover Letter:\", metadata={'source': '/content/ashwin.txt'}), Document(page_content=\"My Cover Letter:\\n\\nDear Hiring Manager,\\nI am a graduate student pursuing my Master of Science in Computer Science (MS Thesis)\\ndegree from Virginia Tech with a concentration in coursework and thesis in Machine Learning\\nand Explainable AI. I am looking for opportunities in the field of Machine Learning with a focus of\\nextracting key insights from data and operationalize ML based applications and also delve into\\nthe flexibility in discipline of work and excitement to jump in new areas of technology. I'm highly\\ninterested in building software solutions for professionals turning billions of data points into\\nmeaningful answers.\\nMy undergraduate and current graduate coursework was analogous to an evolutionary\\ncomputing model - in order to achieve a proficiency in Data Science it was essential to obtain a set\\nof skills in a progressive fashion.Through taking courses related to Machine Learning and Data\\nScience and doing various competitions on Kaggle and becoming a Kaggle Notebooks expert I\\nburrowed myself into the fundamentals of ML and AI. After gaining enough basic knowledge I\\nwent on to intern Outreach Corporation as a Machine Learning Engineering Intern where I was\\nresponsible for for developing a template engine project to help Data Scientists and Machine\\nLearning Engineers at Outreach to use templates to deploy any NLP model online inorder for\\nthem to avoid writing redundant boiler plate code. I delivered an Online Inference Solution with a\\ngRPC based Microservice in Golang serving NLP based models viz. BERT, ROBERTA and\\nDISTILBERT for topic detection, question detection, action analysis and sentiment analysis that\\nreduced the online inference deployment time from 6-7 days to simply 2 hours.\\nBefore that I worked at Mindbowser Inc where I was working on a Facial Expression Recognition\\napplication based on Deep Transfer Learning that was intended to record customer behaviour\\nduring meetings/sales pitches for improving CRM by detecting and classifying facial expressions\\ninto one of the seven fundamental categories of emotions. I also wrote a script to store the\\nmeeting metadata in a mongoDB database.\\nIn my graduate coursework I took the courses related to Data Science viz, Data Analytics,\\nIntroduction To Deep Learning, Natural language Processing Information Storage and\\nExtraction and Software Engineering wherin I have worked on and solve various problems in\\nDS related to coupled with how they can be implemented and deployed in the industry. In my\\ngraduate thesis I am working in the domain of Explainable AI in with Dr. Mohammed Seyam of\\nVirginia Tech. I Intend to study how the output of Data based models can be explained or\\ninterpreted to reinforce trust between smallholder farmers and the underlying intelligent systems\\n. I have also produced two research papers in the similar field. My love for writing and interest in\\nData Science constantly inspires me to elucidate ML concepts which I compile from time to time\\non Medium and my own Blog.\\nIn all of the qualities mentioned above and in my Resume and the history in my Cover Letter, I\\nbelieve that my willingness to learn new things and dedication to find a solution is what sets me\\napart from my peers. I love working and collaborating in a team with different diverse opinions\\nand have always welcomed constructive criticism. Infact much of the knowledge and experience I\\nhave, have been the result of listening to the advice of people better than me.I am curious and\\nadventurous in my journey to understanding Machine Learning which is what wakes me up in the\\nmorning with a jolt and I am ready to take the next challenge in my industrial experience with an\\nopportunity to work for your esteemed organization for which I shall be utterly grateful.\\nSincerely,\\nAshwin Rachha.\\n\\nMy Resume:\", metadata={'source': '/content/ashwin.txt'}), Document(page_content='ASHWIN RACHHA\\nhttps://ashwinrachha.github.io/home/ | linkedin.com/in/ashwinrachha | +1-5408248748 | ashwinr@vt.edu | Blog\\nEDUCATION:\\nVirginia Tech, M.S. in Computer Science, Blacksburg, VA | GPA 4.0/4.0 August 2021 - August 2023\\nPICT, B.E. in Computer Science, Pune, India | GPA 8.70/10.00 July 2016 - November 2020\\nSKILLS:\\nProgramming Languages and Databases : Python, Go, Java, MySQL, Sqlite, MongoDB.\\nFrameworks: Github, Docker, Kubernetes, CircleCI, Spark, Mlflow, Pytorch, Tensorflow, Numpy, Pandas, LLMs, Langchain.\\nCloud and Web Development : AWS, GCP, HuggingFace, Replit, Jupyter, HTML, FastAPI, Flask, CSS, React, Streamlit, Gradio.\\nEXPERIENCE:\\nOutreach, Seattle , Washington | Machine Learning Engineering Intern (ML Platform Team) May 2022 – August 2022\\n● Delivered an Online Inference Solution, templatized for command line usage saving Data Scientist’s time (3-4 days to 30 minutes)\\nfor serving NLP based models viz. BERT, ROBERTA and DISTILBERT for topic detection, question detection, action analysis and\\nsentiment analysis. End product resulted in a deployed application online via Kubernetes manifests on Outreach Staging Environment.\\n● Wrote Pyspark pipelines for ingesting data, preprocessing, tokenization, prediction and postprocessing of text data.\\n● Wrote Python scripts to instantiate NLP model binaries in the ONNX format on the NVIDIA Triton Inference Server and\\nalternatively a Python and Go based microservice packaged in a Dockerfile customizable automatically through cli by Data Scientists.\\n● Wrote tests for the application service as well as the inference service via CircleCI configuration files.\\nUNAR Labs , Portland, Maine | Machine Learning Engineer June 2023 – Present\\n● Responsible for writing a new backend for UNAR Labs empowering multisensory information access for visually impaired.\\n● Architected and optimized pipelines for data preprocessing, model training, and inference, leveraging frameworks such as\\nOpenCV, PyTorch, Transformers, FastAPI, Distributed Servers Docker, GCP, HuggingFace - full stack data science.\\nMindbowser Inc, Pune , India | Machine Learning Intern December 19 – May 2020\\n● Implemented a Facial Expression recognition application to detect and classify expressions during an ongoing meeting for CRM.\\n● Trained a Convolutional Neural Network VGG-19 transfer learning model as the final classifier in Pytorch achieving an accuracy of\\n73% on the validation set. Integrated the application with a MongoDB database storing metadata and images in GridFS format.\\n● Packaged the code in a python based executable which could be instantiated with a button click on the desktop as an application.\\n● The project is in beta testing at Volkswagen and Bajaj India.\\nVirginia Tech, Blacksburg , Virginia | Graduate Teaching Assistant July 2022 - May 2023\\n● Graduate Teaching Assistant for Software Engineering and Data Analytics. Assisted students, explained concepts, conducted grading,\\ncollaborated on course development, and recognized for teaching skills. Proficient in Java, Kotlin, Python, R, data analytics tools,\\nvisualization, ML concepts, and software engineering.\\nPROJECTS:\\nCode Explainer. [Python, Pytorch, Transformers, Huggingface, Gradio] LIVE WEBSITE CODE\\n● A transformer based Natural Language Explanation project trained using LLMs(Large Language Models - CodeT5) which converts\\nany given python code and generates explanations for it like ChatGPT. Fine tuned using Pytorch and deployed on Huggingface spaces.\\nVideo to Mp3 Converter Distributed System. [Python, Kubernetes, Docker, Flask, MySQL, RabbitMQ, MongoDB] CODE\\n● Built an authenticated service for uploading, downloading and converting media files using microservices architecture facilitated by\\nPika and RabbitMQ. The application relies on Flask, PyMongo and GridFS to handle interactions with MongoDB.\\nPUBLICATIONS:\\nExplainable AI in Education : Current Trends, Challenges and Opportunities | (IEEE SouthEastCon ) | PAPER April 2023', metadata={'source': '/content/ashwin.txt'}), Document(page_content='PUBLICATIONS:\\nExplainable AI in Education : Current Trends, Challenges and Opportunities | (IEEE SouthEastCon ) | PAPER April 2023\\n● Published a paper on the current state-of-the-art in XAI approaches for education, highlighting their distinctive stipulations and\\naddressing the gaps in existing research. Presented an algorithmic, non-algorithmic and design choice based framework for XAI.\\nACHIEVEMENTS:\\nKaggle Expert and Blogging | Kaggle | Medium Blogging Since November 2020\\n● Achieved the designation of kaggle expert (top 1%) in notebooks category by contributing code in data science competitions.', metadata={'source': '/content/ashwin.txt'}), Document(page_content='### Education', metadata={'source': '/content/ashwin.txt'}), Document(page_content='- **How did you fare in high school mathematics, physical sciences and computing? Which were your strengths and which most enjoyable? How did you rank, competitively, in these subjects?**\\n    - Back when I was growing up in India, from class 1st to class 6th I was not that great in Mathematics. In class 6th I was almost going to fail in the class. It was at that time that I was sent to a more competitive school where students were hungry and passionate about learning new concepts and excel at school. That kind of competitive spirit and eagerness to learn gave them and along with them, me, excitement about learning new concepts. So from class 6th to class 12th I fared excellent in high school mathematics and physics. I was not yet introduced to computing in my High School it was not until I started my bachelors degree did I get to delve my hands into Computers. I had also secured a top 2000 position in my State Level aptitude examination which tested Physics, Mathematics and Chemistry skills in High School i.e class 12th (right before university admissions)\\n- **What sort of high school student were you? What would your high school peers remember you for, if we asked them?**\\n    - Because I was shifting schools a lot, I had a lot of changes in my personality throughout my schooling. In the later years of my high school I was a pretty introvert yet welcoming person. Going through a personal skin tragedy myself, a big highlight about my personality was being empathetic about someone else’s potential reservations or inhibitions and trying to comfort them in my own way. So I became a very good listener and bonded with people whenever they needed to be heard or needed to listen to something inspiration. I was always throwing inspirational quotes and stuff to say at them, which I would like to believe made them feed slightly better about themselves, if not entirely.\\n- **In languages and the arts at high school, what were your strongest subjects and how did you rank in those among your school peers?**\\n    - I was fanatic about English Literature and loved reading classics and writing some poetry of my own in my high school. I was a big fan of William Wordsworth, Oscar Wilde, William Blake, Lord Byron and also a big short story writer’s fan ranging from Anton Chekhov to Leo Tolstoy to Guy de Maupassant. I believed because I was an introvert and could not open myself up to the the world, I can connect very well with the world of literature and hence put a lot of focus over there. Some of my early works can be found on this website https://ashwinrachha.wordpress.com/ .\\n    \\n    **********************************************************************************************************************************************************************PS . I had written that blog when I was 15 so a lot of time has passed since then. My writing has surely matured.********************************************************************************************************************************************************************** \\n    \\n- **Please outline some high school achievements considered exceptional by peers and staff members.**\\n    - I had won an inter-school poetry competition which was quite a prestigious achievement for me. The competition was held between prestigious schools from all over the state and I had won the first prize for my poem “Umbra” which I had written myself which talked about racism in the orient. It was the first time when I truly felt important as being lauded by staff and peers meant quite a lot as initially nobody had any idea about my writings and my passion for writing.\\n- **Which degree and university did you choose, and why?**', metadata={'source': '/content/ashwin.txt'}), Document(page_content='- **Which degree and university did you choose, and why?**\\n    - I chose to study Computer Science during university. I was interested in AI and Machine learning right from that time. My interest in computer science and artificial intelligence sprung from a number of sources. My original interest was incited by a novel innovation in chess, IBM’s deep blue, a chess engine that defeated the world champion of chess. The fact that machines could be automated to not only take decisions like a human being\\n    but to dare to go beyond the extent of the ordinary norms of the generational technology captivated me. I found having a Bachelor’s degree in Computer Science indispensable for my future aspirations and hence took admission at the Pune Institute of Computer Technology. Pune Institute of Computer Technology is one of the most renowned colleges in my hometown i.e Pune.\\n    - After getting comprehensive experience with working in the intersection of Software Development, Machine Learning Engineering and Data Science Development, I found having a Master’s degree indispensable for my future growth. I was always interested in research and found out that Virginia Tech offered some of the best courses and research tracks in the country and I applied to the university. I was one of the fortunate 50 something people to get an admit and I started my educational experience here in the US. My thesis right now is in Explainable Artificial Intelligence in Education and I am loving every aspect of my graduate education here at Virginia Tech.\\n- **What did you enjoy most about your time at university?**\\n    - I thoroughly enjoyed the collaborative and hands-on learning environment at Virginia Tech. The coursework was challenging, but working on group projects and applying what I learned to real-world projects allowed me to deepen my understanding of computer science. I also appreciated the support from my professors and peers, and the opportunities to engage in cutting-edge research. There was also an inclusive environment for international students at Virginia Tech which I was a part of so it did not seem like I am not a part of this, it very much felt like home.\\n- **Which university courses did you perform best at? How did you rank in your degree?**\\n    - I performed very well in NLP based courses in my University which were Natural Language Processing and Information Storage and Retrieval. I got an A in both the subjects and our team also got the best project prize for the project “Virginia Tech Search Engine” in which we had employed a search engine on Virginia Tech pages using an Inverted Index data structure.\\n- **Outside of class, what were your interests and where did you spend your time?**\\n    - I spent most of my free time playing chess. I am very much passionate about the game and I although I do not have a FIDE rating my unofficial chess rating is around 2000 which is almost equivalent to that of a National Master. I love playing small blitz (3 min) games online with opponents and study the intricacies of the game. I also follow chess championships and study players (Ofcourse my all time favorites being Magnus Carlsen and Vishy Anand)\\n- **What did you achieve at university that you consider exceptional?**\\n    - I recently got my Paper “Explainable Artificial Intelligence in Education : Current Trends, Challenges and Opportunities” in IEEE SouthEastCon a conference where leading researchers are going to present their work in a variety of fields related to Computer Science in Orlando Florida in April. After an Intensive 1.5 of work in the field I was able to publish and present a framework related to explainability for black box models deployed in the field of CS education. This took a lot of time and patience on my behalf.', metadata={'source': '/content/ashwin.txt'}), Document(page_content='- I would also like to point out another achievement that I would like say is exceptional (personally). During the first half of my first semester it was exceptionally difficult to make connections and find jobs in this country. Because of this I had a mental health situation that I am sure most of us have suffered some or the other time. Upon introspection I found out there are some flaws in me that need to be addressed i.e connecting with people, being personalble, presenting myself and my skills to the world (thats why I started blogging : https://medium.com/@ashwin_rachha    and https://ashwinrachha.github.io/blogpost/) and I finally got a very nice opportunity to intern at Outreach Corporation in Seattle as a Machine Learning Engineer.', metadata={'source': '/content/ashwin.txt'}), Document(page_content='### **Career development**', metadata={'source': '/content/ashwin.txt'}), Document(page_content='- **Please describe any experience as a professional software engineer, working on commercial products.**\\n    - During my time at Outreach, I had the opportunity to work as a Machine Learning Engineering Intern, where I gained hands-on experience in developing online inference solutions. I was part of a dynamic team i.e the Data Science Team and was responsible for delivering solutions that could serve NLP based models for various tasks like topic detection, question detection, sentiment analysis, and action analysis. I utilized my skills in programming languages like Python and bash scripts to instantiate NLP models in their ONNX flavor on the NVIDIA Triton Inference server.  I was also involved in writing Python pipelines for data ingestion, preprocessing, tokenization, prediction and postprocessing of text data. Additionally, I had to learn Go for this internship as most of Outreach’s codebase is in Golang and I wrote a Golang microservice communicate with the models instantiated on the server using docker compose. I also had the experience of working with tools and frameworks like Docker, Kubernetes, CircleCI, Pytorch, and Tensorflow, among others. I was proud to have deployed the application online via Kubernetes manifests and was responsible for writing tests for the application and inference services. This was my first commercial experience with developing container based technology and using container orchestration platform. I also templatized the project to be generically applied by data scientists, reducing the time for development to deployment from several days to just a couple of hours. Overall, working at Outreach was a great learning experience for me and provided me with a wealth of knowledge in the field of machine learning and software development\\n- **Please describe any experience as a software engineer working on internal company projects rather than commercial products.**\\n    - I had this kind of opportunity to work on an internal project at Mindbowser Inc. During my time at Mindbowser, I had the opportunity to work as a Software Engineering Intern in the field of Machine Learning. I was able to gain hands-on experience in developing a Facial Expression Recognition application that aimed to detect and classify expressions during meetings. This project was a challenging but rewarding experience as it allowed me to learn and apply various cutting-edge technologies. Since it was a POC based project and the company did not have any other data scientist to work with me, I had to make a lot of decisions and that taught me the decisions you take now may severely affect you in the future. I had the chance to train a proof of concept Convolutional Neural Network model using transfer learning, and I was able to achieve an accuracy of 73% on the validation set. This project involved using PyTorch and MongoDB, and I was able to package the code in a Python based executable. I also had the opportunity to integrate the code with a Flask application and Docker, making it easier to deploy the application as a web application. Overall, my experience at Mindbowser was a great opportunity for me to grow both personally and professionally. I was able to work on a real-life project and develop my skills in various aspects of software development, including machine learning, database integration, and application deployment. This experience has given me the confidence and ability to tackle complex projects in the future. But that being said there are many things I could have done differently. Now that I have gained some comprehensive experience in the field of ML I can say for sure that I could have made tremendous changes to the way I did things back then in 2019.\\n- **Describe your skill in your preferred development language, and how you attained it.**', metadata={'source': '/content/ashwin.txt'}), Document(page_content='- **Describe your skill in your preferred development language, and how you attained it.**\\n    - I have given a lot of time to hone my Deep Learning skills particularly working in the development of Language Models in Natural Language Processing. Associating my previous As per my resume, I have hands-on experience in NLP (Natural Language Processing) with Python. My work as a Machine Learning Engineering Intern at [Outreach.io](http://outreach.io/), Seattle, Washington involved delivering an online inference solution with NLP based models (BERT, ROBERTA, and DISTILBERT) for various NLP tasks like topic detection, question detection, action analysis, and sentiment analysis. Additionally, I wrote Python pipelines for data ingestion, preprocessing, tokenization, prediction and postprocessing of text data.\\n    - I also have written a Paper on “Detecting Insincere Questions with Text - A Transfer Learning Approach” (https://arxiv.org/abs/2012.07587) Where I compare the performance of various transformers on detecting Insincere Questions on content based websites such as Quora, StackOverflow, Reddit etc. I have also worked on a NLP-based web application \"HemingWay\" (https://share.streamlit.io/rachhaashwin/one_stop_nlp/main)  that summarizes, analyzes, and paraphrases text. These projects demonstrate my understanding and knowledge of the NLP domain.\\n    - My experience as a Software Engineering Intern (Machine Learning) at Mindbowser, Pune, India, involved implementing a facial expression recognition application that used a proof-of-concept Convolutional Neural Network (VGG-19) trained using Pytorch. These projects demonstrate my proficiency in using Python for NLP and Deep Learning tasks.\\n    \\n- **What are your strengths as a software engineer?**\\n    - I have not had an extensive experience as a Software Developer so I might take a hit on the points for my technical jargon over here but I would say time management and organizing myself and my tasks are the biggest strengths of me as a Software Engineer. Having had minor and major failures in many projects in my past I can say I am definitely a better planner now than I ever was regarding the requirements and design of systems I want to work on.\\n- **What experience do you have with Linux system administration? What is the largest group of servers you have helped operate?**\\n    - I do not have a lot of direct interaction with Linux system administration. However, I have used Bash scripts in my work as a Machine Learning Engineering Intern at [Outreach](http://outreach.io/) and have also used Docker and Kubernetes in deploying my applications. I am also extensively familiar with all linux commands and working on linux operating systems throughout my bachelors and my internships.\\n- **What experience do you have with site reliability engineering, keeping production services online and available?**\\n    - I don\\'t have any direct experience with site reliability engineering but I have experience with deploying applications using Kubernetes which is an open-source system for automating deployment, scaling, and management of containerized applications. During my internship at Outreach, I deployed the application online using Kubernetes manifests on the Outreach Staging Environment. This experience showcases my understanding of containerization and deployment which are key components of site reliability engineering. Additionally, I have experience with CircleCI, which is a continuous integration and delivery platform, and I wrote tests for the application and inference services, demonstrating my knowledge of testing and ensuring the stability of applications.', metadata={'source': '/content/ashwin.txt'}), Document(page_content='### **Experience**', metadata={'source': '/content/ashwin.txt'}), Document(page_content=\"- **Describe your level of experience in Python, and how you have attained it.**\\n    - I have a solid foundation in Python programming, having worked with it for a number of years. During that time, I've gained a deep understanding of its core concepts, such as data structures, algorithms, and OOP. I've also had the opportunity to use it in various projects, both as an individual contributor and as part of a team.\\n    - One of the key ways I've honed my skills in Python is through hands-on experience. I've built and maintained a number of software applications and scripts, which have allowed me to gain practical knowledge of the language. For example, I've developed applications for automating various tasks, data processing, and web scraping. This has given me a good understanding of how to implement different features and how to optimize performance.\\n    - In addition to practical experience, I've also made a point of staying up-to-date with the latest developments in the Python community. I've attended meetups, read online tutorials and blogs, and participated in online forums. This has allowed me to learn from others, share my own experiences, and stay ahead of the curve when it comes to new libraries, tools, and best practices.\\n    \\n    Overall, I believe that my combination of hands-on experience, continuous learning, and deep understanding of the language make me a skilled and experienced Python programmer.\\n    \\n- **When did you start working on Linux? Describe your level of experience as a user & developer on Linux.**\\n    - I started working on Linux right from my undergraduate degree. Since my childhood I only knew about the Windows operating system and it wasnt until I came to University I realized the importance of Open Source and Open Source Operating Systems and what beauty Linux was. We were introduced to the Linux operating system and got acquainted with all the basic linux commands which I use till now.\\n- **Describe your experience with container technologies (Docker, LXD, Kubernetes, etc)**\\n    - Throughout my career, I have had extensive experience working with container technologies, specifically Docker and Kubernetes. I have used Docker for packaging and deploying applications as containers, allowing for efficient and consistent deployment across various environments. I have also worked with Kubernetes, a leading open-source platform for automating the deployment, scaling, and management of containerized applications. I have used Kubernetes to manage and orchestrate containers, ensuring the reliability and availability of my applications in production environments. I have attained this experience through hands-on projects and continuously keeping up to date with the latest developments in this field\\n- **How do you prefer to drive documentation for your products?**\\n    - I prefer to drive documentation for my products in a way that is organized and easily accessible. This typically involves creating a clear outline or table of contents that outlines the various components and features of the product, and then writing detailed descriptions for each. I also like to include diagrams and illustrations to help explain complex concepts, and to make the documentation visually appealing. Additionally, I believe in regularly updating and maintaining the documentation to ensure that it stays current and relevant. Overall, my goal is to create documentation that is comprehensive, user-friendly, and useful for all stakeholders, including developers, end users, and other stakeholders.\\n- **How do you think about quality in your products?**\", metadata={'source': '/content/ashwin.txt'}), Document(page_content=\"- **How do you think about quality in your products?**\\n    - When it comes to my products, quality is a top priority. I believe that a high-quality product not only meets customer expectations, but it also reflects positively on my skills as a professional. I approach quality from multiple angles, including careful code development, thorough testing, and regular evaluations of customer feedback. I continuously strive to improve my processes and techniques to ensure that I am delivering the best possible experience for my users. Ultimately, I view quality as an ongoing journey, and I am committed to continuously learning and improving in this area.\\n- **Describe a case where it was very difficult to test code you were writing, but you found a reliable way to do it.**\\n    - I recall a time when I was working on The Facial Expression Recognition project where testing the code was extremely challenging because Machine Learning testing or more precisely Computer Vision Testing is significantly different than Software Testing. It was a demanding task as the code was interdependent on multiple systems and any small change would result in unexpected outcomes. The dataset that the model was trained on was also biased and there was no other additional data stores to get the relevant dataset from. So I ended up creating a custom dataset for myself and wrote unit tests for the prediction on my custom dataset for Facial Recognition i.e detection as well as Expression Recognition. I didn't give up and went on to find a reliable way to test the code. I started by breaking down the code into smaller, manageable parts and then created a testing plan for each part. I also made use of automated testing tools, which allowed me to test the code efficiently and quickly. Moreover, I worked closely with my team and sought feedback from them, which helped me to identify the areas that needed more attention. In the end, my persistence and determination paid off as I was able to develop a robust testing mechanism that ensured the code was reliable and of high quality.\\n- **What would you like to achieve in career development and skills development?**\\n    - In my career journey, I aim to constantly strive for growth and improvement. In the future I want to be a leader in MLOPs i.e Operationalizing Machine Learning Models and I want to see myself as a Tech lead or a Product lead or an entrepreneur who is managing a revolutionary idea. I want to challenge myself by taking on new and diverse projects that will broaden my skill set and deepen my knowledge in the field. Additionally, I hope to take on leadership roles and mentor others, sharing the lessons I have learned and helping others to achieve their own career development goals. Ultimately, my ultimate career goal is to make a positive impact in my field and leave a lasting legacy.\", metadata={'source': '/content/ashwin.txt'}), Document(page_content='### **Context**', metadata={'source': '/content/ashwin.txt'}), Document(page_content=\"- **Are you involved in open source software?**\\n    - No I have not ever contributed to open source software.\\n- **Describe any significant contributions to open source (with links where possible), if any**\\n    - I do not have contribution to open source software but I do routinely write about Technical blogs. Two of my Blog Lists can be found below:\\n    \\n    [Ashwin Rachha - Medium](https://medium.com/@ashwin_rachha)\\n    \\n    [blogpost](https://ashwinrachha.github.io/blogpost/)\\n    \\n- **What do you think are the key ingredients of a successful open source project?**\\n    - From my perspective, the key ingredients of a successful open source project include a strong and dedicated community of contributors, clear and effective communication and collaboration among team members, a well-defined vision and set of goals, high-quality and well-documented code, and a responsive and flexible development process that incorporates feedback and contributions from the wider community. Additionally, it is important for the project to have a robust system for issue tracking, testing and continuous integration, and for project leadership to make informed decisions about prioritizing new features and bug fixes. In order to foster a thriving community and build trust among contributors and users, it is also important for the project to have transparent governance and decision-making processes, and for the project leaders to prioritize inclusivity and diversity.\\n- **Why do you most want to work for Canonical?**\\n    - Canonical is a leading technology company that provides open-source software solutions, with a focus on serving the needs of its global clients. The company is known for its innovative approach to software development, and for its commitment to quality, reliability, and security.\\n    - The most driving reason for me to work for Canonical is opportunity to work on cutting-edge technologies and to be part of a team of experts in the field. Additionally, Canonical's commitment to open-source development can be attractive to those who value collaboration, transparency, and community involvement. I see a lot of MLOps based blogs on the Canonical page and the mission of Canonical to maintain and work on MLOps based tools such as Kubeflow is something that exictes me very much. Furthermore, working for a successful and established company in the technology sector can provide opportunities for career growth and skill development.\\n- **Which other companies are building the sort of products you would like to work on?**\\n    - There are many companies that are working on the products I would love to work on but they are on a bit of a sidetrack. One such company is OpenAI which just released their CHATGPT model. Although the model performs exceptionally well, the model serving is somewhat not that great and the model is mostly on downtown as they had not expected so many users to interact with the system. I would have loved to work on the infrastructure team of deploying the ChatGPT model.\\n- **Which companies have the most interesting approach to devops and site reliability engineering?**\\n    - I do not have much information about different companies employing interesting devops principles but here is what I could gather. Based of my own research, some companies that are often mentioned for their innovative approach to devops and SRE include:\\n    - Google\\n    - Amazon Web Services (AWS)\\n    - Netflix\\n    - Etsy\\n    - Facebook\\n    - Microsoft\\n    - Airbnb\\n    - Dropbox\\n    - Uber\\n    \\n    These companies have implemented unique approaches to devops and SRE, such as adopting principles of chaos engineering, extensive use of automation and data-driven decision making, and prioritizing culture and collaboration between development and operations teams. It's important to research and understand a company's specific approach to devops and SRE before considering joining their team.\", metadata={'source': '/content/ashwin.txt'}), Document(page_content=\"- **What do you think could raise the bar for site reliability engineering, globally?**\\n    - \\n    \\n    There are several things that could raise the bar for site reliability engineering globally. Some of these include:\\n    \\n    1. Adoption of modern technologies and practices: The use of cloud computing, containerization, and other modern technologies can help improve the reliability and scalability of systems.\\n    2. Collaboration between development and operations teams: Effective collaboration between development and operations teams can ensure that systems are designed, built, and operated in a way that prioritizes reliability.\\n    3. Investment in training and development: Organizations should invest in training and development programs for their SRE and devops teams to keep them up-to-date with the latest tools, techniques, and best practices.\\n    4. Emphasis on continuous improvement: SRE teams should continuously monitor and assess the performance of their systems and make improvements to ensure that they are delivering the best possible reliability and availability.\\n    5. Embracing a culture of reliability: Organizations should promote a culture that values reliability and encourages employees to strive for it in all aspects of their work.\\n- **What do you think Canonical needs to improve in its engineering and products?**\\n    - Canonical should continuously strive to improve their engineering and products in order to meet the changing needs of their customers, stay ahead of their competition, and remain relevant in the market. Some common areas that the company should focus on improving may include increasing the speed of product development, improving the quality and reliability of products, enhancing user experience, and adopting new technologies and approaches to stay ahead of the curve. To determine specific areas that Canonical may need to improve, I would suggest conducting research and gathering feedback from customers, industry experts, and other stakeholders.\\n- **Who do you think are key competitors to Canonical? How do you think Canonical should plan to win that race?**\\n    - Red Hat I believe is one big Competitor to Canonical. The three big players are Ubuntu Server (of Canonical), SUSE Linux Enterprise, and Red Hat Enterprise Linux. It's subjective to say that one company is better than the other, as each has its own strengths and weaknesses. However, some people may view Red Hat and SUSE as better than Canonical because they have a strong reputation in the enterprise Linux market and offer comprehensive enterprise-level support and services. To improve, Canonical can focus on expanding its enterprise offerings and services, further developing its cloud and IoT offerings, and continuing to innovate and improve its products to meet the needs of its users and customers. Additionally, it can also focus on expanding its partnerships and collaborations with other companies and organizations to increase its reach and impact in the industry.\", metadata={'source': '/content/ashwin.txt'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(pages)"
      ],
      "metadata": {
        "id": "hMFEGOJkFQe7",
        "outputId": "424531ac-174a-45d2-fc3e-883104e539c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "hMFEGOJkFQe7",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Vector Store using OpenAI"
      ],
      "metadata": {
        "id": "QpTi97ZFON4b"
      },
      "id": "QpTi97ZFON4b"
    },
    {
      "cell_type": "code",
      "source": [
        "# This is where we create the vector database in memory and load the embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
        "vector_store = FAISS.from_documents(pages, embeddings)"
      ],
      "metadata": {
        "id": "uAUwOemzYzhD"
      },
      "id": "uAUwOemzYzhD",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save vector store to files and download/save in another location for reuse"
      ],
      "metadata": {
        "id": "VOdewvujOuaP"
      },
      "id": "VOdewvujOuaP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the files `to local local disk. We will need to save this and upload it to our GitHub repro for use with the Streamlit UIs\n",
        "vector_store.save_local(\"/content/profile_data\")"
      ],
      "metadata": {
        "id": "PaSz8jG3jF0Q"
      },
      "id": "PaSz8jG3jF0Q",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To load the Vector Store from files:"
      ],
      "metadata": {
        "id": "aZBxnIlnPHLs"
      },
      "id": "aZBxnIlnPHLs"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datastore\n",
        "from langchain.vectorstores import FAISS\n",
        "if os.path.exists(\"/content/profile_data\"):\n",
        "  vector_store = FAISS.load_local(\n",
        "      \"/content/profile_data\",\n",
        "      embeddings\n",
        "      )\n",
        "else:\n",
        "  print(f\"Missing files. Upload index.faiss and index.pkl files to data_store directory first\")\n"
      ],
      "metadata": {
        "id": "vPpoCQTdDiSX"
      },
      "id": "vPpoCQTdDiSX",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "35f99145",
      "metadata": {
        "id": "35f99145"
      },
      "source": [
        "# Query using the vector store\n",
        "## Set up the chat model and specific prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "32a49412",
      "metadata": {
        "id": "32a49412"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template=\"\"\"Use the following pieces of context to answer the users question.\n",
        "Write the answer in first person. You are a bot representing a person's resume and are answering about that person in first person.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "3018f865",
      "metadata": {
        "id": "3018f865",
        "outputId": "bb12e719-34f2-4ae6-cef5-66aa0ac4b2ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I have experience writing a Go based microservice to communicate with an inference server via gRPC requests and responses. I have also dockerized the microservice solution and used it to communicate with the inference service. Thanks for asking!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "question = \"Whats your experience with Golang?\"\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "chain_type_kwargs = {\"prompt\": prompt}\n",
        "\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "Always say \"thanks for asking!\" at the end of the answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vector_store.as_retriever(),\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use the chain to query"
      ],
      "metadata": {
        "id": "eNjGnZME_DNU"
      },
      "id": "eNjGnZME_DNU"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "032a47f8",
      "metadata": {
        "id": "032a47f8",
        "outputId": "6f6b0716-a447-41ea-9460-df31cd4122c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I have worked on various projects in Python, including developing applications for automating tasks, data processing, web scraping, and natural language processing. These projects have allowed me to gain practical experience and deepen my understanding of the language. I have also continuously learned and stayed up-to-date with the latest developments in the Python community. Thanks for asking!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "question = \"what projects have you done in Python?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print Answer"
      ],
      "metadata": {
        "id": "RdVmv7HYiMWA"
      },
      "id": "RdVmv7HYiMWA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print Sources"
      ],
      "metadata": {
        "id": "_8Bejc5ziKqN"
      },
      "id": "_8Bejc5ziKqN"
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Can you explain me Text Summarization Medium article in brief?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ],
      "metadata": {
        "id": "SAjOZ0fE7DUl",
        "outputId": "d5404b7a-b774-4771-a539-630b6b3a868c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "id": "SAjOZ0fE7DUl",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Text Summarization Medium article discusses the implementation of a text summarization project using gRPC and Go programming language. The author describes the process of writing a gRPC-based microservice in Go to communicate with an inference server and generate summaries of text data. The article also provides code snippets and examples for reference. Thanks for asking!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "b1677b440931f40d89ef8be7bf03acb108ce003de0ac9b18e8d43753ea2e7103"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0UqlAAxTXnGF",
        "1mHZRgDKXv1r",
        "aZBxnIlnPHLs"
      ]
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}